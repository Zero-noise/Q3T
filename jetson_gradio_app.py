#!/usr/bin/env python3
# coding=utf-8
"""
Minimal Gradio app for deploying Qwen3-TTS on Jetson Orin.

Supports Base / CustomVoice / VoiceDesign models based on the checkpoint.
"""

from __future__ import annotations

import argparse
import importlib
import json
import os
import re
import subprocess
import sys
import threading
import time
import uuid
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import gradio as gr
import numpy as np
import torch
from transformers.generation.logits_process import LogitsProcessorList

from log_config import get_logger, setup_logging

logger = get_logger(__name__)

_STARTUP_CWD = Path.cwd().resolve()

class _NanClampLogitsProcessor:
    def __call__(self, input_ids, scores):
        if torch.isnan(scores).any() or torch.isinf(scores).any():
            scores = torch.nan_to_num(scores, nan=-1e4, posinf=-1e4, neginf=-1e4)
        return scores

def _add_local_qwen_repo() -> None:
    env_root = os.getenv("QWEN3_TTS_ROOT")
    candidates = []
    if env_root:
        candidates.append(env_root)
    candidates.append(str(Path(__file__).resolve().parent / "Qwen3-TTS"))
    for c in candidates:
        if c and Path(c).is_dir():
            if c not in sys.path:
                sys.path.insert(0, c)
            return


_add_local_qwen_repo()

from qwen_tts import Qwen3TTSModel


def _format_bytes(num_bytes: Optional[int]) -> str:
    if num_bytes is None:
        return "unknown"
    if num_bytes <= 0:
        return "0 B"
    units = ["B", "KiB", "MiB", "GiB", "TiB"]
    size = float(num_bytes)
    for u in units:
        if size < 1024 or u == units[-1]:
            return f"{size:.2f} {u}"
        size /= 1024.0
    return f"{size:.2f} TiB"


def _get_default_download_root() -> Path:
    env_root = os.getenv("QWEN3_TTS_DOWNLOAD_DIR")
    if env_root:
        return Path(env_root).expanduser().resolve()
    return _STARTUP_CWD


def _get_default_download_dir(repo_id: str) -> Path:
    safe_repo = repo_id.replace("/", "__")
    return _get_default_download_root() / safe_repo


def _coerce_int(value: Optional[float], default: int) -> int:
    try:
        if value is None:
            return default
        if isinstance(value, float) and np.isnan(value):
            return default
        return int(value)
    except Exception:
        return default


def _coerce_float(value: Optional[float], default: float) -> float:
    try:
        if value is None:
            return default
        if isinstance(value, float) and np.isnan(value):
            return default
        return float(value)
    except Exception:
        return default


def _read_meminfo() -> Dict[str, int]:
    meminfo: Dict[str, int] = {}
    try:
        with open("/proc/meminfo", "r", encoding="utf-8") as f:
            for line in f:
                parts = line.strip().split(":")
                if len(parts) != 2:
                    continue
                key = parts[0].strip()
                value = parts[1].strip().split()
                if not value:
                    continue
                try:
                    meminfo[key] = int(value[0]) * 1024
                except ValueError:
                    continue
    except OSError:
        pass
    return meminfo


def _check_swap() -> Dict[str, Any]:
    result: Dict[str, Any] = {"enabled": False, "total_bytes": 0, "used_bytes": 0, "entries": []}
    try:
        with open("/proc/swaps", "r", encoding="utf-8") as f:
            lines = [ln.strip() for ln in f.readlines() if ln.strip()]
    except OSError:
        return result

    if len(lines) <= 1:
        return result

    total_kib = 0
    used_kib = 0
    entries = []
    for ln in lines[1:]:
        parts = ln.split()
        if len(parts) < 5:
            continue
        filename, _, size_kib, used_kib_entry, _ = parts[:5]
        try:
            size_kib = int(size_kib)
            used_kib_entry = int(used_kib_entry)
        except ValueError:
            continue
        total_kib += size_kib
        used_kib += used_kib_entry
        entries.append({"path": filename, "size_kib": size_kib, "used_kib": used_kib_entry})

    result["enabled"] = total_kib > 0
    result["total_bytes"] = total_kib * 1024
    result["used_bytes"] = used_kib * 1024
    result["entries"] = entries
    return result


def _check_cuda_mem() -> Dict[str, Optional[int]]:
    if not torch.cuda.is_available():
        return {"total": None, "free": None, "used": None}
    if hasattr(torch.cuda, "mem_get_info"):
        free, total = torch.cuda.mem_get_info()
        return {"total": int(total), "free": int(free), "used": int(total - free)}
    return {"total": None, "free": None, "used": None}


class JetsonMonitor:
    """Parse tegrastats output for real-time Jetson hardware monitoring."""

    def __init__(self):
        self._data: Dict[str, Any] = {}
        self._lock = threading.Lock()
        self._process: Optional[subprocess.Popen] = None
        self._thread: Optional[threading.Thread] = None
        self._running = False

    def start(self) -> None:
        if self._running:
            return
        self._running = True
        self._thread = threading.Thread(target=self._poll_loop, daemon=True)
        self._thread.start()

    def stop(self) -> None:
        self._running = False
        if self._process:
            try:
                self._process.terminate()
            except Exception:
                logger.debug("Failed to terminate tegrastats process", exc_info=True)

    def _poll_loop(self) -> None:
        """Try tegrastats first, fall back to sysfs readings."""
        try:
            self._process = subprocess.Popen(
                ["tegrastats", "--interval", "2000"],
                stdout=subprocess.PIPE,
                stderr=subprocess.DEVNULL,
                text=True,
            )
            for line in iter(self._process.stdout.readline, ""):
                if not self._running:
                    break
                self._parse_tegrastats(line.strip())
        except (FileNotFoundError, PermissionError):
            # tegrastats not available or no permission, use sysfs fallback
            while self._running:
                self._read_sysfs()
                time.sleep(2)

    def _parse_tegrastats(self, line: str) -> None:
        """Parse a single tegrastats output line."""
        data: Dict[str, Any] = {}

        # RAM: 3456/7620MB
        ram_match = re.search(r"RAM\s+(\d+)/(\d+)MB", line)
        if ram_match:
            data["ram_used_mb"] = int(ram_match.group(1))
            data["ram_total_mb"] = int(ram_match.group(2))

        # SWAP: 0/8192MB
        swap_match = re.search(r"SWAP\s+(\d+)/(\d+)MB", line)
        if swap_match:
            data["swap_used_mb"] = int(swap_match.group(1))
            data["swap_total_mb"] = int(swap_match.group(2))

        # GR3D_FREQ 45%
        gr3d_match = re.search(r"GR3D_FREQ\s+(\d+)%", line)
        if gr3d_match:
            data["gpu_util_pct"] = int(gr3d_match.group(1))

        # CPU temperatures: CPU@45.5C or cpu@45.5C
        cpu_temp_match = re.search(r"[Cc][Pp][Uu]@([\d.]+)C", line)
        if cpu_temp_match:
            data["cpu_temp_c"] = float(cpu_temp_match.group(1))

        # GPU temperature: GPU@42C or gpu@42C
        gpu_temp_match = re.search(r"[Gg][Pp][Uu]@([\d.]+)C", line)
        if gpu_temp_match:
            data["gpu_temp_c"] = float(gpu_temp_match.group(1))

        # VDD_IN power: VDD_IN 4500mW or VDD_IN 4500/5000
        vdd_match = re.search(r"VDD_IN\s+(\d+)(?:mW|/\d+)", line)
        if vdd_match:
            data["power_mw"] = int(vdd_match.group(1))

        with self._lock:
            self._data.update(data)

    def _read_sysfs(self) -> None:
        """Fallback: read temperatures from sysfs thermal zones."""
        data: Dict[str, Any] = {}
        thermal_base = Path("/sys/devices/virtual/thermal")
        if thermal_base.exists():
            for tz in sorted(thermal_base.glob("thermal_zone*")):
                try:
                    tz_type = (tz / "type").read_text().strip().lower()
                    temp_raw = (tz / "temp").read_text().strip()
                    temp_c = int(temp_raw) / 1000.0
                    if "gpu" in tz_type:
                        data["gpu_temp_c"] = temp_c
                    elif "cpu" in tz_type or "soc" in tz_type:
                        data.setdefault("cpu_temp_c", temp_c)
                except (OSError, ValueError):
                    continue

        with self._lock:
            self._data.update(data)

    def get_data(self) -> Dict[str, Any]:
        with self._lock:
            return dict(self._data)

    def format_summary(self) -> str:
        d = self.get_data()
        if not d:
            return "Jetson ç›‘æ§: ç­‰å¾…æ•°æ®..."

        lines = []
        # Temperatures
        gpu_temp = d.get("gpu_temp_c")
        cpu_temp = d.get("cpu_temp_c")
        if gpu_temp is not None or cpu_temp is not None:
            temp_parts = []
            if gpu_temp is not None:
                temp_parts.append(f"GPU {gpu_temp:.0f}Â°C")
            if cpu_temp is not None:
                temp_parts.append(f"CPU {cpu_temp:.0f}Â°C")
            lines.append(f"æ¸©åº¦: {' | '.join(temp_parts)}")

        # GPU utilization
        gpu_util = d.get("gpu_util_pct")
        if gpu_util is not None:
            lines.append(f"GPU åˆ©ç”¨ç‡: {gpu_util}%")

        # Power
        power = d.get("power_mw")
        if power is not None:
            lines.append(f"åŠŸè€—: {power/1000:.1f} W")

        # RAM
        ram_used = d.get("ram_used_mb")
        ram_total = d.get("ram_total_mb")
        if ram_used is not None and ram_total is not None:
            lines.append(f"RAM: {ram_used}/{ram_total} MB ({ram_used*100//ram_total}%)")

        # Swap
        swap_used = d.get("swap_used_mb")
        swap_total = d.get("swap_total_mb")
        if swap_used is not None and swap_total is not None:
            lines.append(f"Swap: {swap_used}/{swap_total} MB")

        return "\n".join(lines) if lines else "Jetson ç›‘æ§: æ— æ•°æ®"


class InferenceTracker:
    """Track inference latency and compute RTF (Real-Time Factor)."""

    def __init__(self, max_history: int = 100):
        self._history: List[Dict[str, float]] = []
        self._max_history = max_history
        self._lock = threading.Lock()

    def record(self, latency_s: float, text_len: int, audio_len_s: float) -> None:
        rtf = latency_s / audio_len_s if audio_len_s > 0 else float("inf")
        entry = {
            "latency_s": latency_s,
            "text_len": text_len,
            "audio_len_s": audio_len_s,
            "rtf": rtf,
            "timestamp": time.time(),
        }
        with self._lock:
            self._history.append(entry)
            if len(self._history) > self._max_history:
                self._history = self._history[-self._max_history:]

    def format_summary(self) -> str:
        with self._lock:
            history = list(self._history)

        if not history:
            return "æ¨ç†ç»Ÿè®¡: æš‚æ— æ•°æ®"

        n = len(history)
        avg_latency = sum(e["latency_s"] for e in history) / n
        avg_rtf = sum(e["rtf"] for e in history) / n
        last = history[-1]

        lines = [
            f"æ¨ç†ç»Ÿè®¡ (å…± {n} æ¬¡):",
            f"  å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}s | å¹³å‡ RTF: {avg_rtf:.2f}",
            f"  æœ€è¿‘ä¸€æ¬¡: {last['latency_s']:.2f}s | RTF {last['rtf']:.2f} | "
            f"æ–‡æœ¬ {last['text_len']} å­— | éŸ³é¢‘ {last['audio_len_s']:.1f}s",
        ]
        return "\n".join(lines)


def _check_quantization_status(model: Optional[Any] = None) -> str:
    """Detect if model contains quantized layers and return status string."""
    if model is None:
        return "é‡åŒ–çŠ¶æ€: æ¨¡å‹æœªåŠ è½½"

    # Check for quantize_config.json in model directory
    total_linear = 0
    quantized_count = 0
    quant_types = set()

    inner_model = getattr(model, "model", model)
    talker = getattr(inner_model, "talker", inner_model)

    for m in talker.modules():
        cls_name = type(m).__name__
        if isinstance(m, torch.nn.Linear):
            total_linear += 1
        elif any(kw in cls_name.lower() for kw in ("int4", "int8", "quant", "affine", "dynamic")):
            quantized_count += 1
            quant_types.add(cls_name)

    if quantized_count > 0:
        types_str = ", ".join(sorted(quant_types))
        return f"é‡åŒ–çŠ¶æ€: {quantized_count} å±‚å·²é‡åŒ– (ç±»å‹: {types_str})"
    elif total_linear > 0:
        return f"é‡åŒ–çŠ¶æ€: FP16/FP32 (æœªé‡åŒ–, {total_linear} Linear å±‚)"
    else:
        return "é‡åŒ–çŠ¶æ€: æ— æ³•æ£€æµ‹"


# Global monitor and tracker instances
_jetson_monitor: Optional[JetsonMonitor] = None
_inference_tracker: Optional[InferenceTracker] = None


def _get_monitor() -> JetsonMonitor:
    global _jetson_monitor
    if _jetson_monitor is None:
        _jetson_monitor = JetsonMonitor()
        _jetson_monitor.start()
    return _jetson_monitor


def _get_tracker() -> InferenceTracker:
    global _inference_tracker
    if _inference_tracker is None:
        _inference_tracker = InferenceTracker()
    return _inference_tracker


def _cuda_brief_info() -> str:
    if not torch.cuda.is_available():
        return "CUDA available: False"
    try:
        idx = torch.cuda.current_device()
        name = torch.cuda.get_device_name(idx)
    except Exception:
        logger.debug("Failed to query CUDA device info", exc_info=True)
        idx = "unknown"
        name = "unknown"
    return f"CUDA available: True | current_device={idx} | name={name}"


def _infer_model_device(model: torch.nn.Module) -> str:
    try:
        p = next(model.parameters())
        return str(p.device)
    except Exception:
        logger.debug("Cannot infer device from parameters, falling back to attr", exc_info=True)
        dev = getattr(model, "device", None)
        return str(dev) if dev is not None else "unknown"


def _move_speech_tokenizer(st, device: str, dtype: Optional[torch.dtype]) -> None:
    if st is None:
        return
    target_device = torch.device(device)
    target_dtype = dtype
    if target_device.type == "cpu" and target_dtype in (torch.float16, torch.bfloat16):
        target_dtype = torch.float32
    if getattr(st, "model", None) is not None:
        if target_dtype is not None:
            st.model = st.model.to(device=target_device, dtype=target_dtype)
        else:
            st.model = st.model.to(device=target_device)
    st.device = target_device


# æ”¯æŒçš„ Qwen3-TTS æ¨¡å‹åˆ—è¡¨
SUPPORTED_MODELS = [
    "Qwen/Qwen3-TTS-12Hz-0.6B-Base",
    "Qwen/Qwen3-TTS-12Hz-0.6B-CustomVoice",
    "Qwen/Qwen3-TTS-12Hz-1.7B-Base",
    "Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice",
    "Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign",
]


def _validate_model_dir(model_dir: Path) -> Tuple[bool, bool]:
    has_config = (model_dir / "config.json").exists()
    has_weights = any(model_dir.glob("*.safetensors")) or any(model_dir.glob("*.bin"))
    return has_config, has_weights


def _get_hf_cache_dirs() -> List[Path]:
    """è·å– HuggingFace å¸¸è§çš„ç¼“å­˜ç›®å½•åˆ—è¡¨"""
    cache_dirs = []

    # 1. ç¯å¢ƒå˜é‡æŒ‡å®šçš„ç¼“å­˜ç›®å½•
    hf_home = os.getenv("HF_HOME")
    if hf_home:
        cache_dirs.append(Path(hf_home) / "hub")

    hf_cache = os.getenv("HUGGINGFACE_HUB_CACHE")
    if hf_cache:
        cache_dirs.append(Path(hf_cache))

    transformers_cache = os.getenv("TRANSFORMERS_CACHE")
    if transformers_cache:
        cache_dirs.append(Path(transformers_cache))

    # 2. é»˜è®¤ç¼“å­˜ç›®å½• (åŸºäºå½“å‰ç”¨æˆ·ä¸»ç›®å½•)
    home = Path.home()
    default_dirs = [
        home / ".cache" / "huggingface" / "hub",
        home / ".cache" / "huggingface" / "transformers",
        home / ".huggingface" / "hub",
    ]
    cache_dirs.extend(default_dirs)

    # 3. å»é‡å¹¶åªè¿”å›å­˜åœ¨ä¸”å¯è®¿é—®çš„ç›®å½•
    seen = set()
    result = []
    for d in cache_dirs:
        try:
            d = d.resolve()
            if d not in seen and d.exists():
                seen.add(d)
                result.append(d)
        except (PermissionError, OSError):
            # è·³è¿‡æ— æƒé™è®¿é—®çš„ç›®å½•
            continue
    return result


def _find_model_in_hf_cache(repo_id: str) -> Optional[Path]:
    """åœ¨ HuggingFace ç¼“å­˜ç›®å½•ä¸­æŸ¥æ‰¾æ¨¡å‹"""
    # HuggingFace ç¼“å­˜ä½¿ç”¨ models--org--name æ ¼å¼
    cache_folder_name = "models--" + repo_id.replace("/", "--")

    for cache_dir in _get_hf_cache_dirs():
        try:
            model_cache = cache_dir / cache_folder_name
            if model_cache.exists():
                # æ£€æŸ¥ snapshots ç›®å½•
                snapshots = model_cache / "snapshots"
                if snapshots.exists():
                    # è¿”å›æœ€æ–°çš„ snapshot
                    snapshot_dirs = sorted(snapshots.iterdir(), key=lambda x: x.stat().st_mtime, reverse=True)
                    for snap in snapshot_dirs:
                        if snap.is_dir():
                            # éªŒè¯æ˜¯å¦æœ‰å¿…è¦çš„æ–‡ä»¶
                            has_config, has_weights = _validate_model_dir(snap)
                            if has_config and has_weights:
                                return snap
        except (PermissionError, OSError):
            # è·³è¿‡æ— æƒé™è®¿é—®çš„ç›®å½•
            continue
    return None


def _check_model_downloaded(checkpoint: str) -> Dict[str, Any]:
    # 1. æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„
    if os.path.exists(checkpoint):
        ckpt_path = Path(checkpoint)
        if ckpt_path.is_file():
            return {
                "status": "local_file",
                "path": str(ckpt_path),
                "error": "checkpoint å¿…é¡»æ˜¯ç›®å½•æˆ– HuggingFace repo id",
            }
        if ckpt_path.is_dir():
            has_config, has_weights = _validate_model_dir(ckpt_path)
            status = "local_dir" if (has_config and has_weights) else "local_dir_invalid"
            return {
                "status": status,
                "path": str(ckpt_path),
                "has_config": has_config,
                "has_weights": has_weights,
                "error": "æ¨¡å‹ç›®å½•ç¼ºå°‘ config.json æˆ–æƒé‡æ–‡ä»¶" if status == "local_dir_invalid" else None,
            }
        return {"status": "local_missing", "path": str(ckpt_path)}

    # 2. å°è¯•ä½¿ç”¨ huggingface_hub çš„ local_files_only æ¨¡å¼
    try:
        from huggingface_hub import snapshot_download

        repo_dir = snapshot_download(
            repo_id=checkpoint,
            local_files_only=True,
            allow_patterns=["*.safetensors", "*.bin", "config.json", "generation_config.json", "*.json"],
        )
        repo_dir = Path(repo_dir)
        has_config, has_weights = _validate_model_dir(repo_dir)
        if has_config and has_weights:
            return {"status": "cached", "path": str(repo_dir)}
        return {
            "status": "cached_invalid",
            "path": str(repo_dir),
            "has_config": has_config,
            "has_weights": has_weights,
            "error": "ç¼“å­˜æ¨¡å‹ç¼ºå°‘ config.json æˆ–æƒé‡æ–‡ä»¶",
        }
    except Exception:
        logger.debug("huggingface_hub local cache lookup failed for '%s'", checkpoint, exc_info=True)

    # 3. æ£€æŸ¥å½“å‰ç›®å½•ä¸‹çš„é»˜è®¤ä¸‹è½½ä½ç½®
    try:
        local_dir = _get_default_download_dir(checkpoint)
        if local_dir.exists():
            has_config, has_weights = _validate_model_dir(local_dir)
            status = "local_dir" if (has_config and has_weights) else "local_dir_invalid"
            return {
                "status": status,
                "path": str(local_dir),
                "has_config": has_config,
                "has_weights": has_weights,
                "error": "æ¨¡å‹ç›®å½•ç¼ºå°‘ config.json æˆ–æƒé‡æ–‡ä»¶" if status == "local_dir_invalid" else None,
            }
    except Exception:
        logger.debug("Default download dir lookup failed for '%s'", checkpoint, exc_info=True)

    # 4. æ‰‹åŠ¨æœç´¢ HuggingFace ç¼“å­˜ç›®å½•
    found_path = _find_model_in_hf_cache(checkpoint)
    if found_path:
        return {"status": "cached", "path": str(found_path)}

    return {"status": "not_cached", "error": "æ¨¡å‹æœªåœ¨æœ¬åœ°æ‰¾åˆ°"}


def _ensure_output_dir(output_dir: str) -> str:
    path = Path(output_dir).expanduser().resolve()
    path.mkdir(parents=True, exist_ok=True)
    return str(path)


def _save_wav(wav: np.ndarray, sr: int, output_dir: str, prefix: str) -> str:
    ts = time.strftime("%Y%m%d_%H%M%S")
    name = f"{prefix}_{ts}_{uuid.uuid4().hex[:6]}.wav"
    out_path = os.path.join(output_dir, name)
    try:
        import soundfile as sf

        sf.write(out_path, wav, sr)
    except Exception:
        logger.debug("soundfile unavailable, falling back to scipy.io.wavfile", exc_info=True)
        try:
            from scipy.io import wavfile

            wav_int16 = np.clip(wav, -1.0, 1.0)
            wav_int16 = (wav_int16 * 32767.0).astype(np.int16)
            wavfile.write(out_path, sr, wav_int16)
        except Exception as e:
            logger.error("Failed to save audio to %s: %s", out_path, e, exc_info=True)
            raise RuntimeError(f"Failed to save audio: {e}") from e
    return out_path


def _system_check_summary(checkpoint: str, output_dir: str, model: Any = None) -> str:
    meminfo = _read_meminfo()
    mem_total = meminfo.get("MemTotal")
    mem_avail = meminfo.get("MemAvailable")
    mem_used = mem_total - mem_avail if mem_total and mem_avail else None

    swap = _check_swap()
    cuda = _check_cuda_mem()
    model_status = _check_model_downloaded(checkpoint)

    lines = []
    lines.append(f"æ¨¡å‹æ£€æŸ¥: {model_status.get('status')}")
    if model_status.get("status") in {"local_dir", "local_dir_invalid", "cached_invalid"}:
        lines.append(f"- è·¯å¾„: {model_status.get('path')}")
        lines.append(f"- config.json: {model_status.get('has_config')}")
        lines.append(f"- æƒé‡æ–‡ä»¶: {model_status.get('has_weights')}")
        if model_status.get("error"):
            lines.append(f"- é”™è¯¯: {model_status.get('error')}")
    elif model_status.get("status") in {"local_file", "cached"}:
        lines.append(f"- è·¯å¾„: {model_status.get('path')}")
        if model_status.get("error"):
            lines.append(f"- é”™è¯¯: {model_status.get('error')}")
    elif model_status.get("status") == "not_cached":
        lines.append("- æœªåœ¨æœ¬åœ°ç¼“å­˜æ£€æµ‹åˆ°æ¨¡å‹ï¼Œå¯æå‰ç¦»çº¿ä¸‹è½½")

    lines.append(f"å†…å­˜: { _format_bytes(mem_used) } / { _format_bytes(mem_total) } (used/total)")
    if swap["enabled"]:
        lines.append(f"Swap: { _format_bytes(swap['used_bytes']) } / { _format_bytes(swap['total_bytes']) }")
    else:
        lines.append("Swap: æœªå¯ç”¨")
    if cuda["total"]:
        lines.append(
            f"CUDA æ˜¾å­˜: { _format_bytes(cuda['used']) } / { _format_bytes(cuda['total']) }"
        )
    lines.append(f"è¾“å‡ºç›®å½•: {output_dir}")

    # Jetson monitor data
    monitor = _get_monitor()
    mon_summary = monitor.format_summary()
    if "ç­‰å¾…æ•°æ®" not in mon_summary and "æ— æ•°æ®" not in mon_summary:
        lines.append("")
        lines.append(mon_summary)

    # Quantization status
    lines.append("")
    lines.append(_check_quantization_status(model))

    # Inference stats
    tracker = _get_tracker()
    tracker_summary = tracker.format_summary()
    if "æš‚æ— æ•°æ®" not in tracker_summary:
        lines.append("")
        lines.append(tracker_summary)

    return "\n".join(lines)


def _dtype_from_str(s: str) -> torch.dtype:
    s = (s or "").strip().lower()
    if s in ("bf16", "bfloat16"):
        return torch.bfloat16
    if s in ("fp16", "float16", "half"):
        return torch.float16
    if s in ("fp32", "float32"):
        return torch.float32
    raise ValueError(f"Unsupported torch dtype: {s}. Use bfloat16/float16/float32.")


def _maybe_auto_language(lang: str) -> str:
    lang = (lang or "").strip()
    return lang if lang else "Auto"


def _collect_gen_kwargs(max_new_tokens, temperature, top_k, top_p, repetition_penalty) -> Dict[str, Any]:
    mapping = {
        "max_new_tokens": max_new_tokens,
        "temperature": temperature,
        "top_k": top_k,
        "top_p": top_p,
        "repetition_penalty": repetition_penalty,
    }
    return {k: v for k, v in mapping.items() if v is not None}


def _build_logits_processor(enabled: bool) -> Optional[LogitsProcessorList]:
    if not enabled:
        return None
    return LogitsProcessorList([_NanClampLogitsProcessor()])


def _load_tts(args: argparse.Namespace) -> Qwen3TTSModel:
    dtype = _dtype_from_str(args.dtype)
    attn_impl = None if args.no_flash_attn else "flash_attention_2"
    device_map = args.device
    if isinstance(device_map, str) and device_map.startswith("cuda:"):
        parts = device_map.split(":", 1)
        if len(parts) == 2 and parts[1].isdigit():
            try:
                torch.cuda.set_device(int(parts[1]))
            except Exception:
                logger.warning("Failed to set CUDA device %s", parts[1], exc_info=True)
        device_map = "cuda"
    logger.info("device_map=%s (from '%s') | dtype=%s | flash_attn=%s",
                device_map, args.device, dtype, "on" if attn_impl else "off")
    logger.info("%s", _cuda_brief_info())

    # åœ¨åŠ è½½å‰æ¸…ç† GPU ç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        import gc
        gc.collect()

    use_staged = bool(getattr(args, "staged_load", False))
    tokenizer_on_cpu = bool(getattr(args, "tokenizer_on_cpu", False))
    logger.info("staged_load=%s | tokenizer_on_cpu=%s",
                "on" if use_staged else "off", "on" if tokenizer_on_cpu else "off")

    if use_staged and device_map == "cuda":
        logger.info("staged_load path: CPU -> dtype -> GPU")
        tts = Qwen3TTSModel.from_pretrained(
            args.checkpoint,
            device_map="cpu",
            torch_dtype=dtype if dtype not in (torch.float16, torch.bfloat16) else torch.float32,
            attn_implementation=attn_impl,
            low_cpu_mem_usage=True,
        )
        if dtype in (torch.float16, torch.bfloat16):
            tts.model = tts.model.to(dtype=dtype)
        tts.model = tts.model.to("cuda")
        tts.device = torch.device("cuda")
        if hasattr(tts.model, "device"):
            tts.model.device = torch.device("cuda")
        if hasattr(tts.model, "speech_tokenizer"):
            st = getattr(tts.model, "speech_tokenizer", None)
            if tokenizer_on_cpu:
                _move_speech_tokenizer(st, "cpu", dtype)
            else:
                _move_speech_tokenizer(st, "cuda", dtype)
        return tts

    tts = Qwen3TTSModel.from_pretrained(
        args.checkpoint,
        device_map=device_map,
        torch_dtype=dtype,
        attn_implementation=attn_impl,
        low_cpu_mem_usage=True,  # å‡å°‘åŠ è½½æ—¶çš„å†…å­˜å³°å€¼
    )
    if hasattr(tts.model, "speech_tokenizer"):
        st = getattr(tts.model, "speech_tokenizer", None)
        if tokenizer_on_cpu:
            _move_speech_tokenizer(st, "cpu", dtype)
    return tts


def _build_base_ui(tts: Qwen3TTSModel, output_dir: str, save_audio: bool):
    with gr.Tab("è¯­éŸ³å…‹éš† (Base)"):
        with gr.Row():
            with gr.Column(scale=3):
                text = gr.Textbox(label="åˆæˆæ–‡æœ¬", lines=4, placeholder="è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬...", info="æ”¯æŒä¸­è‹±æ–‡æ··åˆ")
                with gr.Row():
                    language = gr.Dropdown(label="è¯­è¨€", choices=["Auto", "Chinese", "English"], value="Auto", allow_custom_value=True, scale=1)
                    xvec_only = gr.Checkbox(label="ä»…ä½¿ç”¨éŸ³è‰²ç‰¹å¾ (æ— éœ€å‚è€ƒæ–‡æœ¬)", value=False, scale=2)
            with gr.Column(scale=2):
                ref_audio = gr.Audio(label="å‚è€ƒéŸ³é¢‘", type="filepath", sources=["upload", "microphone"])
                ref_text = gr.Textbox(label="å‚è€ƒéŸ³é¢‘æ–‡æœ¬", lines=2, placeholder="è¾“å…¥å‚è€ƒéŸ³é¢‘ä¸­è¯´çš„å†…å®¹...", info="å…³é—­ã€Œä»…éŸ³è‰²ã€æ—¶å¿…å¡«")

        with gr.Accordion("ç”Ÿæˆå‚æ•°", open=False):
            with gr.Row():
                max_new_tokens = gr.Slider(label="max_new_tokens", minimum=256, maximum=4096, value=1024, step=64)
                temperature = gr.Slider(label="temperature", minimum=0.1, maximum=2.0, value=0.8, step=0.05)
            with gr.Row():
                top_k = gr.Slider(label="top_k", minimum=1, maximum=100, value=50, step=1)
                top_p = gr.Slider(label="top_p", minimum=0.1, maximum=1.0, value=0.9, step=0.05)
                repetition_penalty = gr.Slider(label="repetition_penalty", minimum=1.0, maximum=2.0, value=1.05, step=0.01)

        gen_btn = gr.Button("ç”Ÿæˆè¯­éŸ³", variant="primary")
        with gr.Row():
            audio_out = gr.Audio(label="ç”Ÿæˆç»“æœ", type="numpy", scale=3)
            status = gr.Textbox(label="çŠ¶æ€", lines=2, scale=1)

        def _infer_base(
            text_in: str,
            lang_in: str,
            ref_audio_path: Optional[str],
            ref_text_in: str,
            xvec_only_in: bool,
            max_new_tokens_in: float,
            temperature_in: float,
            top_k_in: float,
            top_p_in: float,
            repetition_penalty_in: float,
        ) -> Tuple[Optional[Tuple[int, Any]], str]:
            if not ref_audio_path:
                return None, "è¯·å…ˆä¸Šä¼ å‚è€ƒéŸ³é¢‘"
            if not xvec_only_in and not (ref_text_in or "").strip():
                return None, "ICL æ¨¡å¼éœ€è¦å‚è€ƒæ–‡æœ¬"

            gen_kwargs = _collect_gen_kwargs(
                _coerce_int(max_new_tokens_in, 1024),
                _coerce_float(temperature_in, 0.8),
                _coerce_int(top_k_in, 50),
                _coerce_float(top_p_in, 0.9),
                _coerce_float(repetition_penalty_in, 1.05),
            )
            wavs, sr = tts.generate_voice_clone(
                text=text_in,
                language=_maybe_auto_language(lang_in),
                ref_audio=ref_audio_path,
                ref_text=ref_text_in,
                x_vector_only_mode=bool(xvec_only_in),
                **gen_kwargs,
            )
            saved_path = ""
            if save_audio:
                saved_path = _save_wav(wavs[0], sr, output_dir, "base")
            status_msg = f"OK{f' | Saved: {saved_path}' if saved_path else ''}"
            return (sr, wavs[0]), status_msg

        gen_btn.click(
            _infer_base,
            inputs=[
                text,
                language,
                ref_audio,
                ref_text,
                xvec_only,
                max_new_tokens,
                temperature,
                top_k,
                top_p,
                repetition_penalty,
            ],
            outputs=[audio_out, status],
        )


def _build_custom_ui(tts: Qwen3TTSModel, output_dir: str, save_audio: bool):
    speakers = tts.model.get_supported_speakers() or []
    with gr.Tab("é¢„è®¾è§’è‰² (CustomVoice)"):
        with gr.Row():
            with gr.Column(scale=2):
                text = gr.Textbox(label="åˆæˆæ–‡æœ¬", lines=4, placeholder="è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬...", info="æ”¯æŒä¸­è‹±æ–‡æ··åˆ")
                language = gr.Dropdown(label="è¯­è¨€", choices=["Auto", "Chinese", "English"], value="Auto", allow_custom_value=True)
            with gr.Column(scale=1):
                speaker = gr.Dropdown(label="é€‰æ‹©è§’è‰²", choices=speakers, value=speakers[0] if speakers else None, info="æ¨¡å‹å†…ç½®çš„é¢„è®¾è¯´è¯äºº")
                instruct = gr.Textbox(label="é£æ ¼æŒ‡ä»¤ (å¯é€‰)", lines=2, placeholder="ä¾‹å¦‚: å¼€å¿ƒåœ°ã€æ‚„æ‚„åœ°ã€å¿«é€Ÿ...")

        with gr.Accordion("ç”Ÿæˆå‚æ•°", open=False):
            with gr.Row():
                max_new_tokens = gr.Slider(label="max_new_tokens", minimum=256, maximum=4096, value=1024, step=64)
                temperature = gr.Slider(label="temperature", minimum=0.1, maximum=2.0, value=0.8, step=0.05)
            with gr.Row():
                top_k = gr.Slider(label="top_k", minimum=1, maximum=100, value=50, step=1)
                top_p = gr.Slider(label="top_p", minimum=0.1, maximum=1.0, value=0.9, step=0.05)
                repetition_penalty = gr.Slider(label="repetition_penalty", minimum=1.0, maximum=2.0, value=1.05, step=0.01)

        gen_btn = gr.Button("ç”Ÿæˆè¯­éŸ³", variant="primary")
        with gr.Row():
            audio_out = gr.Audio(label="ç”Ÿæˆç»“æœ", type="numpy", scale=3)
            status = gr.Textbox(label="çŠ¶æ€", lines=2, scale=1)

        def _infer_custom(
            text_in: str,
            lang_in: str,
            speaker_in: str,
            instruct_in: str,
            max_new_tokens_in: float,
            temperature_in: float,
            top_k_in: float,
            top_p_in: float,
            repetition_penalty_in: float,
        ) -> Tuple[Optional[Tuple[int, Any]], str]:
            if not speaker_in:
                return None, "è¯·é€‰æ‹© speaker"

            gen_kwargs = _collect_gen_kwargs(
                _coerce_int(max_new_tokens_in, 1024),
                _coerce_float(temperature_in, 0.8),
                _coerce_int(top_k_in, 50),
                _coerce_float(top_p_in, 0.9),
                _coerce_float(repetition_penalty_in, 1.05),
            )
            wavs, sr = tts.generate_custom_voice(
                text=text_in,
                language=_maybe_auto_language(lang_in),
                speaker=speaker_in,
                instruct=instruct_in,
                **gen_kwargs,
            )
            saved_path = ""
            if save_audio:
                saved_path = _save_wav(wavs[0], sr, output_dir, "custom")
            status_msg = f"OK{f' | Saved: {saved_path}' if saved_path else ''}"
            return (sr, wavs[0]), status_msg

        gen_btn.click(
            _infer_custom,
            inputs=[
                text,
                language,
                speaker,
                instruct,
                max_new_tokens,
                temperature,
                top_k,
                top_p,
                repetition_penalty,
            ],
            outputs=[audio_out, status],
        )


def _build_voice_design_ui(tts: Qwen3TTSModel, output_dir: str, save_audio: bool):
    with gr.Tab("é£æ ¼è®¾è®¡ (VoiceDesign)"):
        with gr.Row():
            with gr.Column(scale=2):
                text = gr.Textbox(label="åˆæˆæ–‡æœ¬", lines=4, placeholder="è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬...", info="æ”¯æŒä¸­è‹±æ–‡æ··åˆ")
                language = gr.Dropdown(label="è¯­è¨€", choices=["Auto", "Chinese", "English"], value="Auto", allow_custom_value=True)
            with gr.Column(scale=1):
                instruct = gr.Textbox(label="è¯­éŸ³é£æ ¼æè¿°", lines=4, placeholder="æè¿°ä½ æƒ³è¦çš„å£°éŸ³ç‰¹ç‚¹...\nä¾‹å¦‚:\n- æ¸©æŸ”çš„å¥³å£°ï¼Œè¯­é€Ÿç¼“æ…¢\n- ä½æ²‰æœ‰ç£æ€§çš„ç”·å£°", info="ç”¨è‡ªç„¶è¯­è¨€æè¿°å£°éŸ³ç‰¹ç‚¹")

        with gr.Accordion("ç”Ÿæˆå‚æ•°", open=False):
            with gr.Row():
                max_new_tokens = gr.Slider(label="max_new_tokens", minimum=256, maximum=4096, value=1024, step=64)
                temperature = gr.Slider(label="temperature", minimum=0.1, maximum=2.0, value=0.8, step=0.05)
            with gr.Row():
                top_k = gr.Slider(label="top_k", minimum=1, maximum=100, value=50, step=1)
                top_p = gr.Slider(label="top_p", minimum=0.1, maximum=1.0, value=0.9, step=0.05)
                repetition_penalty = gr.Slider(label="repetition_penalty", minimum=1.0, maximum=2.0, value=1.05, step=0.01)

        gen_btn = gr.Button("ç”Ÿæˆè¯­éŸ³", variant="primary")
        with gr.Row():
            audio_out = gr.Audio(label="ç”Ÿæˆç»“æœ", type="numpy", scale=3)
            status = gr.Textbox(label="çŠ¶æ€", lines=2, scale=1)

        def _infer_design(
            text_in: str,
            lang_in: str,
            instruct_in: str,
            max_new_tokens_in: float,
            temperature_in: float,
            top_k_in: float,
            top_p_in: float,
            repetition_penalty_in: float,
        ) -> Tuple[Optional[Tuple[int, Any]], str]:
            gen_kwargs = _collect_gen_kwargs(
                _coerce_int(max_new_tokens_in, 1024),
                _coerce_float(temperature_in, 0.8),
                _coerce_int(top_k_in, 50),
                _coerce_float(top_p_in, 0.9),
                _coerce_float(repetition_penalty_in, 1.05),
            )
            t0 = time.time()
            wavs, sr = tts.generate_voice_design(
                text=text_in,
                language=_maybe_auto_language(lang_in),
                instruct=instruct_in,
                **gen_kwargs,
            )
            latency = time.time() - t0
            audio_len = len(wavs[0]) / sr if sr > 0 else 0
            _get_tracker().record(latency, len(text_in), audio_len)

            saved_path = ""
            if save_audio:
                saved_path = _save_wav(wavs[0], sr, output_dir, "design")
            rtf = latency / audio_len if audio_len > 0 else 0
            status_msg = f"OK | {latency:.2f}s | RTF {rtf:.2f}{f' | Saved: {saved_path}' if saved_path else ''}"
            return (sr, wavs[0]), status_msg

        gen_btn.click(
            _infer_design,
            inputs=[
                text,
                language,
                instruct,
                max_new_tokens,
                temperature,
                top_k,
                top_p,
                repetition_penalty,
            ],
            outputs=[audio_out, status],
        )


def _scan_all_cached_models() -> List[Dict[str, Any]]:
    """æ‰«ææ‰€æœ‰å·²ç¼“å­˜çš„ Qwen3-TTS æ¨¡å‹"""
    found = []
    for repo_id in SUPPORTED_MODELS:
        result = _check_model_downloaded(repo_id)
        if result["status"] in ("cached", "local_dir"):
            found.append({
                "repo_id": repo_id,
                "path": result.get("path", ""),
                "status": result["status"],
            })
    return found


def _download_model(repo_id: str, local_dir: Optional[str], progress=gr.Progress()) -> str:
    """ä¸‹è½½æ¨¡å‹åˆ°æŒ‡å®šç›®å½•"""
    try:
        from huggingface_hub import snapshot_download
    except ImportError:
        return "é”™è¯¯: è¯·å…ˆå®‰è£… huggingface_hub: pip install huggingface_hub"

    progress(0, desc=f"å¼€å§‹ä¸‹è½½ {repo_id}...")

    try:
        kwargs = {
            "repo_id": repo_id,
            "allow_patterns": [
                "*.safetensors",
                "*.bin",
                "*.pt",
                "*.npz",
                "*.json",
                "*.jsonl",
                "*.txt",
                "*.model",
                "*.vocab",
                "*.tiktoken",
                "*.spm",
                "*.sentencepiece",
                "*.merges",
            ],
        }
        if local_dir and local_dir.strip():
            local_path = Path(local_dir).expanduser().resolve()
        else:
            local_path = _get_default_download_dir(repo_id)
        local_path.mkdir(parents=True, exist_ok=True)
        kwargs["local_dir"] = str(local_path)

        progress(0.1, desc="æ­£åœ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶...")
        result_path = snapshot_download(**kwargs)
        progress(1.0, desc="ä¸‹è½½å®Œæˆ!")
        return f"ä¸‹è½½æˆåŠŸ!\næ¨¡å‹è·¯å¾„: {result_path}\n\nè¯·é‡å¯åº”ç”¨å¹¶ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤:\npython jetson_gradio_app.py {result_path}"
    except Exception as e:
        logger.error("Model download failed for %s: %s", repo_id, e, exc_info=True)
        return f"ä¸‹è½½å¤±è´¥: {str(e)}"


def build_download_ui() -> gr.Blocks:
    """æ„å»ºæ¨¡å‹ä¸‹è½½ç•Œé¢"""
    with gr.Blocks(title="Qwen3-TTS ä¸‹è½½") as demo:
        gr.Markdown("# Qwen3-TTS æ¨¡å‹ä¸‹è½½")
        gr.Markdown("æ£€æµ‹åˆ°æœ¬åœ°æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Œè¯·å…ˆä¸‹è½½æ¨¡å‹")

        # æ˜¾ç¤ºå·²ç¼“å­˜çš„æ¨¡å‹
        cached_models = _scan_all_cached_models()
        if cached_models:
            with gr.Accordion("å·²æ£€æµ‹åˆ°çš„æœ¬åœ°æ¨¡å‹", open=True):
                cached_info = "\n".join([f"- **{m['repo_id']}**\n  è·¯å¾„: `{m['path']}`" for m in cached_models])
                gr.Markdown(cached_info)
                gr.Markdown("å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ç›´æ¥å¯åŠ¨:")
                for m in cached_models:
                    gr.Code(f"python jetson_gradio_app.py {m['path']}", language="bash")

        # ä¸‹è½½æ–°æ¨¡å‹
        with gr.Accordion("ä¸‹è½½æ–°æ¨¡å‹", open=not cached_models):
            with gr.Group():
                model_choice = gr.Dropdown(
                    label="é€‰æ‹©æ¨¡å‹",
                    choices=SUPPORTED_MODELS,
                    value=SUPPORTED_MODELS[0],
                    info="0.6B è½»é‡å¿«é€Ÿï¼Œ1.7B è´¨é‡æ›´é«˜ | Base=è¯­éŸ³å…‹éš†, CustomVoice=é¢„è®¾è§’è‰², VoiceDesign=é£æ ¼æè¿°(ä»…1.7B)"
                )

                default_root = _get_default_download_root()
                gr.Markdown(f"ğŸ“ **ä¸‹è½½ç›®å½•**: `{default_root}`")

                use_custom_dir = gr.Checkbox(label="ä½¿ç”¨è‡ªå®šä¹‰ä¸‹è½½ç›®å½•", value=False)
                custom_dir = gr.Textbox(
                    label="è‡ªå®šä¹‰ç›®å½•",
                    placeholder=f"ä¾‹å¦‚: ~/models/Qwen3-TTS-0.6B",
                    visible=False
                )

            def toggle_custom_dir(use_custom):
                return gr.update(visible=use_custom)

            use_custom_dir.change(toggle_custom_dir, inputs=[use_custom_dir], outputs=[custom_dir])

            download_btn = gr.Button("å¼€å§‹ä¸‹è½½", variant="primary")
            download_status = gr.Textbox(label="ä¸‹è½½çŠ¶æ€", lines=5, interactive=False)

            def do_download(model, use_custom, custom_path, progress=gr.Progress()):
                local_dir = custom_path if use_custom and custom_path.strip() else None
                return _download_model(model, local_dir, progress)

            download_btn.click(
                do_download,
                inputs=[model_choice, use_custom_dir, custom_dir],
                outputs=[download_status]
            )

        # æ‰‹åŠ¨æŒ‡å®šæ¨¡å‹è·¯å¾„
        with gr.Accordion("æ‰‹åŠ¨æŒ‡å®šæœ¬åœ°æ¨¡å‹è·¯å¾„", open=False):
            gr.Markdown("å¦‚æœæ¨¡å‹å·²ç»ä¸‹è½½åˆ°å…¶ä»–ä½ç½®ï¼Œå¯ä»¥ç›´æ¥æŒ‡å®šè·¯å¾„:")
            with gr.Row():
                manual_path = gr.Textbox(
                    label="æ¨¡å‹è·¯å¾„",
                    placeholder="ä¾‹å¦‚: /path/to/Qwen3-TTS-12Hz-0.6B-Base",
                    scale=3
                )
                check_btn = gr.Button("æ£€æŸ¥", scale=1)
            check_result = gr.Textbox(label="æ£€æŸ¥ç»“æœ", lines=3, interactive=False)

            def check_manual_path(path):
                if not path or not path.strip():
                    return "è¯·è¾“å…¥è·¯å¾„"
                result = _check_model_downloaded(path.strip())
                if result["status"] in ("local_dir", "cached"):
                    return f"âœ… æ£€æµ‹åˆ°æœ‰æ•ˆæ¨¡å‹!\nè·¯å¾„: {result.get('path', path)}\n\nå¯åŠ¨å‘½ä»¤:\npython jetson_gradio_app.py {result.get('path', path)}"
                return f"âŒ æœªæ£€æµ‹åˆ°æœ‰æ•ˆæ¨¡å‹\nçŠ¶æ€: {result['status']}\né”™è¯¯: {result.get('error', 'è·¯å¾„ä¸å­˜åœ¨æˆ–ç¼ºå°‘å¿…è¦æ–‡ä»¶')}"

            check_btn.click(check_manual_path, inputs=[manual_path], outputs=[check_result])

        gr.Markdown("---")
        with gr.Accordion("ä½¿ç”¨è¯´æ˜", open=False):
            gr.Markdown("""
**æ¨¡å‹ç±»å‹è¯´æ˜:**
- **Base**: è¯­éŸ³å…‹éš†æ¨¡å‹ï¼Œéœ€è¦å‚è€ƒéŸ³é¢‘
- **CustomVoice**: é¢„å®šä¹‰è¯´è¯äººæ¨¡å‹
- **VoiceDesign**: é€šè¿‡æ–‡å­—æè¿°æ§åˆ¶è¯­éŸ³é£æ ¼

**å‚æ•°è§„æ¨¡è¯´æ˜:**
- **0.6B**: è½»é‡ç‰ˆæœ¬ï¼Œæ¨ç†æ›´å¿«ï¼Œé€‚åˆ Jetson éƒ¨ç½²ï¼ˆä»… Base å’Œ CustomVoiceï¼‰
- **1.7B**: æ——èˆ°ç‰ˆæœ¬ï¼Œè´¨é‡æ›´é«˜ï¼ˆåŒ…å« VoiceDesignï¼‰

ä¸‹è½½å®Œæˆåï¼Œä½¿ç”¨æ˜¾ç¤ºçš„å‘½ä»¤é‡å¯åº”ç”¨ã€‚
            """)

    return demo


def build_demo(
    tts: Qwen3TTSModel,
    checkpoint: str,
    output_dir: str,
    save_audio: bool,
    force_model_type: Optional[str] = None,
) -> gr.Blocks:
    with gr.Blocks(title="Qwen3-TTS") as demo:
        gr.Markdown("# Qwen3-TTS Jetson Orin")
        gr.Markdown("æ–‡æœ¬è½¬è¯­éŸ³æ¼”ç¤º | Text-to-Speech Demo")

        model_type = (force_model_type or getattr(tts.model, "tts_model_type", "")).strip()
        if model_type == "base":
            _build_base_ui(tts, output_dir, save_audio)
        elif model_type == "custom_voice":
            _build_custom_ui(tts, output_dir, save_audio)
        elif model_type == "voice_design":
            _build_voice_design_ui(tts, output_dir, save_audio)
        else:
            gr.Markdown(f"âš ï¸ ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")

        with gr.Accordion("ç³»ç»Ÿç›‘æ§ (System Monitor)", open=False):
            with gr.Row():
                with gr.Column(scale=2):
                    sys_info = gr.Textbox(
                        label="ç³»ç»ŸçŠ¶æ€",
                        lines=10,
                        value=_system_check_summary(checkpoint, output_dir, model=tts),
                    )
                with gr.Column(scale=1):
                    quant_info = gr.Textbox(
                        label="é‡åŒ– & æ¨ç†",
                        lines=10,
                        value=_check_quantization_status(tts) + "\n\n" + _get_tracker().format_summary(),
                    )
            with gr.Row():
                auto_refresh_cb = gr.Checkbox(label="è‡ªåŠ¨åˆ·æ–°", value=False)
                refresh_btn = gr.Button("æ‰‹åŠ¨åˆ·æ–°")

            def _refresh() -> Tuple[str, str]:
                sys_text = _system_check_summary(checkpoint, output_dir, model=tts)
                quant_text = _check_quantization_status(tts) + "\n\n" + _get_tracker().format_summary()
                return sys_text, quant_text

            refresh_btn.click(_refresh, outputs=[sys_info, quant_info])

            # Auto-refresh via Timer if Gradio supports it
            try:
                timer = gr.Timer(value=5, active=False)

                def _auto_tick() -> Tuple[str, str]:
                    return _refresh()

                timer.tick(_auto_tick, outputs=[sys_info, quant_info])

                def _toggle_timer(enabled: bool):
                    return gr.Timer(active=enabled)

                auto_refresh_cb.change(_toggle_timer, inputs=[auto_refresh_cb], outputs=[timer])
            except (AttributeError, TypeError):
                logger.debug("Gradio Timer not supported in this version, auto-refresh disabled")

        gr.Markdown("---")
        gr.Markdown(
            "<center style='color: #888; font-size: 0.85em;'>"
            "âš ï¸ ç”Ÿæˆçš„éŸ³é¢‘ä»…ä¾›æ¼”ç¤ºä½¿ç”¨ï¼Œè¯·å‹¿ç”¨äºéæ³•æˆ–æœ‰å®³ç”¨é€”ã€‚"
            "</center>"
        )
    return demo


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Qwen3-TTS Gradio demo for Jetson Orin.")
    parser.add_argument(
        "checkpoint",
        nargs="?",
        default=None,
        help="Model checkpoint path or HuggingFace repo id. If not provided, will auto-detect or show download UI.",
    )
    parser.add_argument("--device", default="cpu", help="Device for device_map (default: cpu).")
    parser.add_argument(
        "--dtype",
        default="float32",
        choices=["bfloat16", "bf16", "float16", "fp16", "float32", "fp32"],
        help="Torch dtype for loading the model (default: float32).",
    )
    parser.add_argument(
        "--no-flash-attn",
        dest="no_flash_attn",
        action="store_true",
        default=True,
        help="Disable FlashAttention-2 (default: disabled).",
    )
    parser.add_argument(
        "--flash-attn",
        dest="no_flash_attn",
        action="store_false",
        help="Enable FlashAttention-2.",
    )
    parser.add_argument(
        "--staged-load",
        action="store_true",
        default=True,
        help="Staged load for Jetson (CPU -> dtype -> GPU).",
    )
    parser.add_argument(
        "--no-staged-load",
        dest="staged_load",
        action="store_false",
        help="Disable staged load.",
    )
    parser.add_argument(
        "--tokenizer-on-cpu",
        action="store_true",
        default=True,
        help="Keep speech tokenizer on CPU (reduce GPU memory).",
    )
    parser.add_argument(
        "--tokenizer-on-gpu",
        dest="tokenizer_on_cpu",
        action="store_false",
        help="Move speech tokenizer to GPU.",
    )
    parser.add_argument("--ip", default="0.0.0.0", help="Gradio server bind IP.")
    parser.add_argument("--port", type=int, default=8000, help="Gradio server port.")
    parser.add_argument("--share", action="store_true", help="Create a public Gradio link.")
    parser.add_argument("--concurrency", type=int, default=4, help="Gradio queue concurrency.")
    parser.add_argument("--ssl-certfile", default=None, help="Path to SSL cert file for HTTPS.")
    parser.add_argument("--ssl-keyfile", default=None, help="Path to SSL key file for HTTPS.")
    parser.add_argument("--output-dir", default="outputs", help="Directory to save generated audio.")
    parser.add_argument("--no-save", action="store_true", help="Disable saving generated audio.")
    parser.add_argument(
        "--auto-detect",
        action="store_true",
        help="Auto-detect and use the first available cached model.",
    )
    parser.add_argument(
        "--backend",
        choices=["torch", "trt"],
        default="torch",
        help="Inference backend. Use 'trt' for TensorRT-LLM INT4 engine.",
    )
    parser.add_argument(
        "--engine-path",
        default=None,
        help="TensorRT-LLM engine path (required when --backend trt).",
    )
    parser.add_argument(
        "--tokenizer-dir",
        default=None,
        help="Tokenizer directory (required when --backend trt).",
    )
    parser.add_argument(
        "--model-type",
        choices=["voice_design"],
        default=None,
        help="Force model type when backend doesn't expose it.",
    )
    return parser


def _scan_local_models() -> List[Dict[str, Any]]:
    """æ‰«æå½“å‰ç›®å½•ä¸‹å·²ä¸‹è½½çš„æ¨¡å‹"""
    found = []
    for repo_id in SUPPORTED_MODELS:
        local_dir = _get_default_download_dir(repo_id)
        if local_dir.exists():
            has_config, has_weights = _validate_model_dir(local_dir)
            if has_config and has_weights:
                found.append({
                    "repo_id": repo_id,
                    "path": str(local_dir),
                    "status": "local_dir",
                })
    return found


def _scan_models_in_directory(directory: str) -> List[Dict[str, Any]]:
    """æ‰«ææŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰æœ‰æ•ˆæ¨¡å‹"""
    found = []
    if not directory or not directory.strip():
        return found

    dir_path = Path(directory).expanduser().resolve()
    if not dir_path.exists() or not dir_path.is_dir():
        return found

    try:
        # æ£€æŸ¥ç›®å½•æœ¬èº«æ˜¯å¦æ˜¯æ¨¡å‹ç›®å½•
        has_config, has_weights = _validate_model_dir(dir_path)
        if has_config and has_weights:
            found.append({
                "name": dir_path.name,
                "path": str(dir_path),
                "type": _detect_model_type(dir_path),
            })
            return found

        # æ‰«æå­ç›®å½•
        for sub in sorted(dir_path.iterdir()):
            if sub.is_dir():
                has_config, has_weights = _validate_model_dir(sub)
                if has_config and has_weights:
                    found.append({
                        "name": sub.name,
                        "path": str(sub),
                        "type": _detect_model_type(sub),
                    })
    except (PermissionError, OSError):
        logger.debug("Cannot scan model directory %s", dir_path, exc_info=True)

    return found


def _detect_model_type(model_dir: Path) -> str:
    """æ£€æµ‹æ¨¡å‹ç±»å‹ (base/custom_voice/voice_design)"""
    config_path = model_dir / "config.json"
    if not config_path.exists():
        return "unknown"

    try:
        import json
        with open(config_path, "r", encoding="utf-8") as f:
            config = json.load(f)
        return config.get("tts_model_type", "unknown")
    except Exception:
        logger.debug("Failed to detect model type from %s", config_path, exc_info=True)
        return "unknown"


def _get_all_model_locations() -> List[str]:
    """è·å–æ‰€æœ‰å¯èƒ½çš„æ¨¡å‹ä½ç½®"""
    locations = []

    # 1. å½“å‰å·¥ä½œç›®å½•
    cwd = str(Path.cwd().resolve())
    locations.append(cwd)

    # 2. ç¯å¢ƒå˜é‡æŒ‡å®šçš„ç›®å½•
    env_root = os.getenv("QWEN3_TTS_DOWNLOAD_DIR")
    if env_root:
        locations.append(str(Path(env_root).expanduser().resolve()))

    # 3. HuggingFace ç¼“å­˜ç›®å½•
    for cache_dir in _get_hf_cache_dirs():
        locations.append(str(cache_dir))

    # 4. å¸¸è§çš„æ¨¡å‹å­˜æ”¾ä½ç½®
    home = Path.home()
    common_dirs = [
        home / "models",
        home / "Models",
        home / ".cache" / "models",
        Path("/models"),
        Path("/data/models"),
    ]
    for d in common_dirs:
        try:
            if d.exists():
                locations.append(str(d.resolve()))
        except (PermissionError, OSError):
            logger.debug("Cannot access model directory %s", d)

    # å»é‡å¹¶ä¿æŒé¡ºåº
    seen = set()
    result = []
    for loc in locations:
        if loc not in seen:
            seen.add(loc)
            result.append(loc)

    return result


def build_lazy_demo(args: argparse.Namespace) -> gr.Blocks:
    """æ„å»ºå»¶è¿ŸåŠ è½½æ¨¡å‹çš„ç•Œé¢ - å…ˆå¯åŠ¨UIï¼ŒååŠ è½½æ¨¡å‹"""
    if args.backend != "torch":
        raise ValueError("Lazy demo only supports --backend torch.")
    # ä½¿ç”¨å¯å˜å®¹å™¨å­˜å‚¨æ¨¡å‹çŠ¶æ€
    state = {"tts": None, "checkpoint": None, "model_type": None, "sanitize_logits": True}
    output_dir = _ensure_output_dir(args.output_dir)
    save_audio = not args.no_save

    # å¯åŠ¨æ—¶æ‰«æå¯èƒ½çš„æ¨¡å‹ä½ç½®
    all_locations = _get_all_model_locations()
    default_location = all_locations[0] if all_locations else str(Path.cwd())

    # æ‰«æé»˜è®¤ä½ç½®çš„æ¨¡å‹
    initial_models = _scan_models_in_directory(default_location)
    initial_model_choices = [f"{m['name']} ({m['type']})" for m in initial_models]
    initial_model_paths = {f"{m['name']} ({m['type']})": m['path'] for m in initial_models}

    with gr.Blocks(title="Qwen3-TTS") as demo:
        gr.Markdown("# Qwen3-TTS Jetson Orin")
        gr.Markdown("æ–‡æœ¬è½¬è¯­éŸ³æ¼”ç¤º | Text-to-Speech Demo")

        # ç”¨äºå­˜å‚¨æ¨¡å‹è·¯å¾„æ˜ å°„çš„çŠ¶æ€
        model_paths_state = gr.State(initial_model_paths)

        # ===== åŒºåŸŸ1: æ¨¡å‹ä¸‹è½½ =====
        with gr.Accordion("ğŸ“¥ æ¨¡å‹ä¸‹è½½", open=False):
            gr.Markdown("ä» HuggingFace ä¸‹è½½ Qwen3-TTS æ¨¡å‹åˆ°æœ¬åœ°")

            with gr.Row():
                with gr.Column(scale=2):
                    download_model_dropdown = gr.Dropdown(
                        label="é€‰æ‹©è¦ä¸‹è½½çš„æ¨¡å‹",
                        choices=SUPPORTED_MODELS,
                        value=SUPPORTED_MODELS[0],
                        info="0.6B è½»é‡å¿«é€Ÿï¼Œ1.7B è´¨é‡æ›´é«˜ | Base=è¯­éŸ³å…‹éš†, CustomVoice=é¢„è®¾è§’è‰², VoiceDesign=é£æ ¼æè¿°(ä»…1.7B)"
                    )
                with gr.Column(scale=2):
                    download_dir_input = gr.Textbox(
                        label="ä¸‹è½½ä½ç½®",
                        value=default_location,
                        placeholder="æ¨¡å‹å°†ä¸‹è½½åˆ°æ­¤ç›®å½•",
                        info="æ¨¡å‹ä¼šä¿å­˜åœ¨è¯¥ç›®å½•ä¸‹çš„å­æ–‡ä»¶å¤¹ä¸­"
                    )
                with gr.Column(scale=1):
                    download_btn = gr.Button("å¼€å§‹ä¸‹è½½", variant="primary", size="lg")

            download_status = gr.Textbox(
                label="ä¸‹è½½çŠ¶æ€",
                lines=2,
                interactive=False,
                value="é€‰æ‹©æ¨¡å‹å’Œä¸‹è½½ä½ç½®åï¼Œç‚¹å‡»ã€Œå¼€å§‹ä¸‹è½½ã€"
            )

            def do_download(repo_id: str, download_dir: str, progress=gr.Progress()):
                if not download_dir.strip():
                    return "è¯·æŒ‡å®šä¸‹è½½ä½ç½®"
                # æ„å»ºç›®æ ‡è·¯å¾„
                safe_name = repo_id.replace("/", "__")
                target_path = str(Path(download_dir).expanduser().resolve() / safe_name)
                return _download_model(repo_id, target_path, progress)

            download_btn.click(
                do_download,
                inputs=[download_model_dropdown, download_dir_input],
                outputs=[download_status]
            )

        # ===== åŒºåŸŸ2: æ¨¡å‹é€‰æ‹©ä¸åŠ è½½ =====
        with gr.Accordion("ğŸ”§ æ¨¡å‹é€‰æ‹©ä¸åŠ è½½", open=True):
            with gr.Row():
                # å·¦ä¾§: æ¨¡å‹ä½ç½®å’Œé€‰æ‹©
                with gr.Column(scale=2):
                    with gr.Group():
                        gr.Markdown("### é€‰æ‹©æ¨¡å‹")

                        model_location_dropdown = gr.Dropdown(
                            label="æ¨¡å‹ä½ç½®",
                            choices=all_locations,
                            value=default_location,
                            allow_custom_value=True,
                            info="é€‰æ‹©æˆ–è¾“å…¥æ¨¡å‹æ‰€åœ¨çš„ç›®å½•"
                        )

                        with gr.Row():
                            scan_btn = gr.Button("ğŸ” æ‰«æ", scale=1)
                            auto_detect_btn = gr.Button("ğŸ”„ è‡ªåŠ¨æ£€æµ‹å…¨éƒ¨ä½ç½®", scale=2)

                        model_select_dropdown = gr.Dropdown(
                            label="æ£€æµ‹åˆ°çš„æ¨¡å‹",
                            choices=initial_model_choices,
                            value=initial_model_choices[0] if initial_model_choices else None,
                            info="é€‰æ‹©è¦åŠ è½½çš„æ¨¡å‹"
                        )

                        model_path_display = gr.Textbox(
                            label="æ¨¡å‹å®Œæ•´è·¯å¾„",
                            value=initial_models[0]['path'] if initial_models else "",
                            interactive=True,
                            info="å¯ç›´æ¥ç¼–è¾‘è·¯å¾„ï¼Œæˆ–é€šè¿‡ä¸Šæ–¹ä¸‹æ‹‰æ¡†é€‰æ‹©"
                        )

                # å³ä¾§: åŠ è½½é€‰é¡¹
                with gr.Column(scale=1):
                    with gr.Group():
                        gr.Markdown("### åŠ è½½é€‰é¡¹")

                        device_input = gr.Dropdown(
                            label="Device",
                            choices=["cpu", "cuda", "cuda:0", "auto"],
                            value=args.device,
                            allow_custom_value=True,
                        )
                        dtype_dropdown = gr.Dropdown(
                            label="ç²¾åº¦ (DType)",
                            choices=["float16", "bfloat16", "float32"],
                            value="float16" if args.dtype in ["fp16", "float16"] else args.dtype,
                        )

                        with gr.Row():
                            flash_attn_checkbox = gr.Checkbox(
                                label="FlashAttn",
                                value=not args.no_flash_attn,
                            )
                            sanitize_logits_checkbox = gr.Checkbox(
                                label="Sanitize",
                                value=True,
                            )

                        with gr.Row():
                            staged_load_checkbox = gr.Checkbox(
                                label="Staged",
                                value=bool(args.staged_load),
                            )
                            tokenizer_cpu_checkbox = gr.Checkbox(
                                label="Tok CPU",
                                value=bool(args.tokenizer_on_cpu),
                            )

            # åŠ è½½æŒ‰é’®
            load_btn = gr.Button("ğŸš€ åŠ è½½æ¨¡å‹", variant="primary", size="lg")

            # çŠ¶æ€æ˜¾ç¤º
            model_status_text = gr.Textbox(
                label="çŠ¶æ€",
                lines=2,
                interactive=False,
                value=f"æ£€æµ‹åˆ° {len(initial_models)} ä¸ªæ¨¡å‹ã€‚é€‰æ‹©æ¨¡å‹åç‚¹å‡»ã€ŒåŠ è½½æ¨¡å‹ã€" if initial_models else "æœªæ£€æµ‹åˆ°æ¨¡å‹ã€‚è¯·å…ˆä¸‹è½½æˆ–æŒ‡å®šæ¨¡å‹ä½ç½®ã€‚"
            )

            # === äº‹ä»¶å¤„ç† ===

            # æ‰«ææŒ‡å®šä½ç½®çš„æ¨¡å‹
            def scan_location(location: str, current_paths: dict):
                models = _scan_models_in_directory(location)
                if not models:
                    return (
                        gr.update(choices=[], value=None),
                        "",
                        current_paths,
                        f"åœ¨ {location} æœªæ‰¾åˆ°æœ‰æ•ˆæ¨¡å‹"
                    )

                choices = [f"{m['name']} ({m['type']})" for m in models]
                paths = {f"{m['name']} ({m['type']})": m['path'] for m in models}

                return (
                    gr.update(choices=choices, value=choices[0]),
                    models[0]['path'],
                    paths,
                    f"æ‰¾åˆ° {len(models)} ä¸ªæ¨¡å‹"
                )

            scan_btn.click(
                scan_location,
                inputs=[model_location_dropdown, model_paths_state],
                outputs=[model_select_dropdown, model_path_display, model_paths_state, model_status_text]
            )

            # ä½ç½®å˜åŒ–æ—¶è‡ªåŠ¨æ‰«æ
            model_location_dropdown.change(
                scan_location,
                inputs=[model_location_dropdown, model_paths_state],
                outputs=[model_select_dropdown, model_path_display, model_paths_state, model_status_text]
            )

            # è‡ªåŠ¨æ£€æµ‹å…¨éƒ¨ä½ç½®
            def auto_detect_all(current_paths: dict):
                all_models = []
                for loc in _get_all_model_locations():
                    models = _scan_models_in_directory(loc)
                    for m in models:
                        m['location'] = loc
                    all_models.extend(models)

                # åŒæ—¶æ‰«æ HuggingFace ç¼“å­˜
                for repo_id in SUPPORTED_MODELS:
                    result = _check_model_downloaded(repo_id)
                    if result["status"] in ("cached", "local_dir"):
                        path = result.get("path", "")
                        if path and not any(m['path'] == path for m in all_models):
                            all_models.append({
                                "name": repo_id.split("/")[-1],
                                "path": path,
                                "type": _detect_model_type(Path(path)),
                                "location": "HuggingFace Cache"
                            })

                if not all_models:
                    return (
                        gr.update(choices=[], value=None),
                        "",
                        current_paths,
                        "æœªæ‰¾åˆ°ä»»ä½•æ¨¡å‹ã€‚è¯·å…ˆä¸‹è½½æ¨¡å‹ã€‚"
                    )

                choices = [f"{m['name']} ({m['type']}) @ {m.get('location', 'local')}" for m in all_models]
                paths = {c: m['path'] for c, m in zip(choices, all_models)}

                return (
                    gr.update(choices=choices, value=choices[0]),
                    all_models[0]['path'],
                    paths,
                    f"å…±æ‰¾åˆ° {len(all_models)} ä¸ªæ¨¡å‹"
                )

            auto_detect_btn.click(
                auto_detect_all,
                inputs=[model_paths_state],
                outputs=[model_select_dropdown, model_path_display, model_paths_state, model_status_text]
            )

            # é€‰æ‹©æ¨¡å‹æ—¶æ›´æ–°è·¯å¾„
            def on_model_select(selection: str, paths: dict):
                if selection and selection in paths:
                    return paths[selection]
                return ""

            model_select_dropdown.change(
                on_model_select,
                inputs=[model_select_dropdown, model_paths_state],
                outputs=[model_path_display]
            )

        # ===== åŒºåŸŸ3: TTS ç”Ÿæˆ (åˆå§‹éšè—) =====
        with gr.Column(visible=False) as tts_area:
            gr.Markdown("---")
            gr.Markdown("## ğŸ™ï¸ è¯­éŸ³åˆæˆ")

            # Base æ¨¡å¼ UI
            with gr.Tab("è¯­éŸ³å…‹éš† (Base)", visible=False) as base_tab:
                with gr.Row():
                    with gr.Column(scale=3):
                        base_text = gr.Textbox(
                            label="åˆæˆæ–‡æœ¬",
                            lines=4,
                            placeholder="è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬...",
                            info="æ”¯æŒä¸­è‹±æ–‡æ··åˆ"
                        )
                        with gr.Row():
                            base_language = gr.Dropdown(
                                label="è¯­è¨€",
                                choices=["Auto", "Chinese", "English"],
                                value="Auto",
                                allow_custom_value=True,
                                scale=1
                            )
                            base_xvec_only = gr.Checkbox(
                                label="ä»…ä½¿ç”¨éŸ³è‰²ç‰¹å¾ (æ— éœ€å‚è€ƒæ–‡æœ¬)",
                                value=False,
                                scale=2
                            )
                    with gr.Column(scale=2):
                        base_ref_audio = gr.Audio(
                            label="å‚è€ƒéŸ³é¢‘",
                            type="filepath",
                            sources=["upload", "microphone"]
                        )
                        base_ref_text = gr.Textbox(
                            label="å‚è€ƒéŸ³é¢‘æ–‡æœ¬",
                            lines=2,
                            placeholder="è¾“å…¥å‚è€ƒéŸ³é¢‘ä¸­è¯´çš„å†…å®¹...",
                            info="å…³é—­ã€Œä»…éŸ³è‰²ã€æ—¶å¿…å¡«"
                        )

                with gr.Accordion("ç”Ÿæˆå‚æ•°", open=False):
                    with gr.Row():
                        base_max_tokens = gr.Slider(label="max_new_tokens", minimum=256, maximum=4096, value=1024, step=64)
                        base_temp = gr.Slider(label="temperature", minimum=0.1, maximum=2.0, value=0.8, step=0.05)
                    with gr.Row():
                        base_top_k = gr.Slider(label="top_k", minimum=1, maximum=100, value=50, step=1)
                        base_top_p = gr.Slider(label="top_p", minimum=0.1, maximum=1.0, value=0.9, step=0.05)
                        base_rep_pen = gr.Slider(label="repetition_penalty", minimum=1.0, maximum=2.0, value=1.05, step=0.01)

                base_gen_btn = gr.Button("ç”Ÿæˆè¯­éŸ³", variant="primary")

                with gr.Row():
                    base_audio_out = gr.Audio(label="ç”Ÿæˆç»“æœ", type="numpy", scale=3)
                    base_status = gr.Textbox(label="çŠ¶æ€", lines=2, scale=1)

            # CustomVoice æ¨¡å¼ UI
            with gr.Tab("é¢„è®¾è§’è‰² (CustomVoice)", visible=False) as custom_tab:
                with gr.Row():
                    with gr.Column(scale=2):
                        custom_text = gr.Textbox(
                            label="åˆæˆæ–‡æœ¬",
                            lines=4,
                            placeholder="è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬...",
                            info="æ”¯æŒä¸­è‹±æ–‡æ··åˆ"
                        )
                        custom_language = gr.Dropdown(
                            label="è¯­è¨€",
                            choices=["Auto", "Chinese", "English"],
                            value="Auto",
                            allow_custom_value=True
                        )
                    with gr.Column(scale=1):
                        custom_speaker = gr.Dropdown(
                            label="é€‰æ‹©è§’è‰²",
                            choices=[],
                            value=None,
                            info="æ¨¡å‹å†…ç½®çš„é¢„è®¾è¯´è¯äºº"
                        )
                        custom_instruct = gr.Textbox(
                            label="é£æ ¼æŒ‡ä»¤ (å¯é€‰)",
                            lines=2,
                            placeholder="ä¾‹å¦‚: å¼€å¿ƒåœ°ã€æ‚„æ‚„åœ°ã€å¿«é€Ÿ..."
                        )

                with gr.Accordion("ç”Ÿæˆå‚æ•°", open=False):
                    with gr.Row():
                        custom_max_tokens = gr.Slider(label="max_new_tokens", minimum=256, maximum=4096, value=1024, step=64)
                        custom_temp = gr.Slider(label="temperature", minimum=0.1, maximum=2.0, value=0.8, step=0.05)
                    with gr.Row():
                        custom_top_k = gr.Slider(label="top_k", minimum=1, maximum=100, value=50, step=1)
                        custom_top_p = gr.Slider(label="top_p", minimum=0.1, maximum=1.0, value=0.9, step=0.05)
                        custom_rep_pen = gr.Slider(label="repetition_penalty", minimum=1.0, maximum=2.0, value=1.05, step=0.01)

                custom_gen_btn = gr.Button("ç”Ÿæˆè¯­éŸ³", variant="primary")

                with gr.Row():
                    custom_audio_out = gr.Audio(label="ç”Ÿæˆç»“æœ", type="numpy", scale=3)
                    custom_status = gr.Textbox(label="çŠ¶æ€", lines=2, scale=1)

            # VoiceDesign æ¨¡å¼ UI
            with gr.Tab("é£æ ¼è®¾è®¡ (VoiceDesign)", visible=False) as design_tab:
                with gr.Row():
                    with gr.Column(scale=2):
                        design_text = gr.Textbox(
                            label="åˆæˆæ–‡æœ¬",
                            lines=4,
                            placeholder="è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬...",
                            info="æ”¯æŒä¸­è‹±æ–‡æ··åˆ"
                        )
                        design_language = gr.Dropdown(
                            label="è¯­è¨€",
                            choices=["Auto", "Chinese", "English"],
                            value="Auto",
                            allow_custom_value=True
                        )
                    with gr.Column(scale=1):
                        design_instruct = gr.Textbox(
                            label="è¯­éŸ³é£æ ¼æè¿°",
                            lines=4,
                            placeholder="æè¿°ä½ æƒ³è¦çš„å£°éŸ³ç‰¹ç‚¹...\nä¾‹å¦‚:\n- æ¸©æŸ”çš„å¥³å£°ï¼Œè¯­é€Ÿç¼“æ…¢\n- ä½æ²‰æœ‰ç£æ€§çš„ç”·å£°\n- æ´»æ³¼çš„æ’­éŸ³è…”",
                            info="ç”¨è‡ªç„¶è¯­è¨€æè¿°å£°éŸ³ç‰¹ç‚¹"
                        )

                with gr.Accordion("ç”Ÿæˆå‚æ•°", open=False):
                    with gr.Row():
                        design_max_tokens = gr.Slider(label="max_new_tokens", minimum=256, maximum=4096, value=1024, step=64)
                        design_temp = gr.Slider(label="temperature", minimum=0.1, maximum=2.0, value=0.8, step=0.05)
                    with gr.Row():
                        design_top_k = gr.Slider(label="top_k", minimum=1, maximum=100, value=50, step=1)
                        design_top_p = gr.Slider(label="top_p", minimum=0.1, maximum=1.0, value=0.9, step=0.05)
                        design_rep_pen = gr.Slider(label="repetition_penalty", minimum=1.0, maximum=2.0, value=1.05, step=0.01)

                design_gen_btn = gr.Button("ç”Ÿæˆè¯­éŸ³", variant="primary")

                with gr.Row():
                    design_audio_out = gr.Audio(label="ç”Ÿæˆç»“æœ", type="numpy", scale=3)
                    design_status = gr.Textbox(label="çŠ¶æ€", lines=2, scale=1)

            # ç³»ç»Ÿç›‘æ§é¢æ¿
            with gr.Accordion("ç³»ç»Ÿç›‘æ§ (System Monitor)", open=False):
                with gr.Row():
                    with gr.Column(scale=2):
                        sys_info = gr.Textbox(label="ç³»ç»ŸçŠ¶æ€", lines=10, value="")
                    with gr.Column(scale=1):
                        quant_info = gr.Textbox(label="é‡åŒ– & æ¨ç†", lines=10, value="")
                with gr.Row():
                    auto_refresh_cb = gr.Checkbox(label="è‡ªåŠ¨åˆ·æ–°", value=False)
                    refresh_btn = gr.Button("æ‰‹åŠ¨åˆ·æ–°")

        gr.Markdown("---")
        gr.Markdown(
            "<center style='color: #888; font-size: 0.85em;'>"
            "âš ï¸ ç”Ÿæˆçš„éŸ³é¢‘ä»…ä¾›æ¼”ç¤ºä½¿ç”¨ï¼Œè¯·å‹¿ç”¨äºéæ³•æˆ–æœ‰å®³ç”¨é€”ã€‚"
            "</center>"
        )

        # ===== åŠ è½½æ¨¡å‹é€»è¾‘ =====
        def load_model_fn(model_path: str, device_in: str, dtype_in: str, flash_attn_in: bool, sanitize_logits_in: bool, staged_load_in: bool, tokenizer_cpu_in: bool):
            nonlocal state

            if not model_path or not model_path.strip():
                return (
                    "è¯·å…ˆé€‰æ‹©æˆ–è¾“å…¥æ¨¡å‹è·¯å¾„",
                    gr.update(visible=False),
                    "",
                    "",
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(choices=[], value=None),
                )

            try:
                checkpoint = model_path.strip()

                # éªŒè¯æ¨¡å‹è·¯å¾„
                model_dir = Path(checkpoint)
                if model_dir.exists():
                    has_config, has_weights = _validate_model_dir(model_dir)
                    if not has_config or not has_weights:
                        return (
                            f"æ— æ•ˆçš„æ¨¡å‹è·¯å¾„: {checkpoint}\nç¼ºå°‘ config.json æˆ–æƒé‡æ–‡ä»¶",
                            gr.update(visible=False),
                            "",
                            "",
                            gr.update(visible=False),
                            gr.update(visible=False),
                            gr.update(visible=False),
                            gr.update(choices=[], value=None),
                        )

                # åº”ç”¨åŠ è½½å‚æ•°
                args.device = (device_in or "").strip() or args.device
                args.dtype = (dtype_in or "").strip() or args.dtype
                args.no_flash_attn = not bool(flash_attn_in)
                args.staged_load = bool(staged_load_in)
                args.tokenizer_on_cpu = bool(tokenizer_cpu_in)

                if args.device.lower().startswith("cpu") and args.dtype.lower() in ("bfloat16", "bf16", "float16", "fp16"):
                    logger.info("CPU æ¨¡å¼ä¸‹å°† dtype è‡ªåŠ¨åˆ‡æ¢ä¸º float32ï¼Œä»¥é¿å… NaN/Inf")
                    args.dtype = "float32"

                state["sanitize_logits"] = bool(sanitize_logits_in)

                # åŠ è½½æ¨¡å‹
                args.checkpoint = checkpoint
                tts = _load_tts(args)
                model_type = getattr(tts.model, "tts_model_type", "")
                logger.info("model_type=%s | model_device=%s", model_type, _infer_model_device(tts.model))

                try:
                    st = getattr(tts.model, "speech_tokenizer", None)
                    st_model = getattr(st, "model", None) if st is not None else None
                    if st_model is not None:
                        logger.info("speech_tokenizer_device=%s", _infer_model_device(st_model))
                except Exception:
                    logger.debug("Failed to query speech_tokenizer device", exc_info=True)

                state["tts"] = tts
                state["checkpoint"] = checkpoint
                state["model_type"] = model_type

                # è·å– speakers (ä»… custom_voice æ¨¡å¼)
                speakers = []
                if model_type == "custom_voice":
                    speakers = tts.model.get_supported_speakers() or []

                status_msg = (
                    f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!\n"
                    f"è·¯å¾„: {checkpoint}\n"
                    f"ç±»å‹: {model_type} | device={args.device} | dtype={args.dtype}"
                )
                sys_check = _system_check_summary(checkpoint, output_dir, model=tts)

                quant_check = _check_quantization_status(tts) + "\n\n" + _get_tracker().format_summary()

                # è¿”å› UI æ›´æ–°
                return (
                    status_msg,  # model_status_text
                    gr.update(visible=True),  # tts_area
                    sys_check,  # sys_info
                    quant_check,  # quant_info
                    gr.update(visible=(model_type == "base")),  # base_tab
                    gr.update(visible=(model_type == "custom_voice")),  # custom_tab
                    gr.update(visible=(model_type == "voice_design")),  # design_tab
                    gr.update(choices=speakers, value=speakers[0] if speakers else None),  # custom_speaker
                )
            except Exception as e:
                logger.error("æ¨¡å‹åŠ è½½å¤±è´¥: %s", e, exc_info=True)
                error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
                return (
                    error_msg,
                    gr.update(visible=False),
                    "",
                    "",
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(choices=[], value=None),
                )

        load_btn.click(
            load_model_fn,
            inputs=[model_path_display, device_input, dtype_dropdown, flash_attn_checkbox, sanitize_logits_checkbox, staged_load_checkbox, tokenizer_cpu_checkbox],
            outputs=[model_status_text, tts_area, sys_info, quant_info, base_tab, custom_tab, design_tab, custom_speaker]
        )

        # åˆ·æ–°ç³»ç»Ÿæ£€æŸ¥
        def refresh_sys_check():
            tts_obj = state.get("tts")
            if state["checkpoint"]:
                sys_text = _system_check_summary(state["checkpoint"], output_dir, model=tts_obj)
            else:
                sys_text = "æ¨¡å‹æœªåŠ è½½"
            quant_text = _check_quantization_status(tts_obj) + "\n\n" + _get_tracker().format_summary()
            return sys_text, quant_text

        refresh_btn.click(refresh_sys_check, outputs=[sys_info, quant_info])

        # Auto-refresh via Timer if Gradio supports it
        try:
            lazy_timer = gr.Timer(value=5, active=False)
            lazy_timer.tick(refresh_sys_check, outputs=[sys_info, quant_info])

            def _toggle_lazy_timer(enabled: bool):
                return gr.Timer(active=enabled)

            auto_refresh_cb.change(_toggle_lazy_timer, inputs=[auto_refresh_cb], outputs=[lazy_timer])
        except (AttributeError, TypeError):
            logger.debug("Gradio Timer not supported in this version, auto-refresh disabled")

        # ===== TTS ç”Ÿæˆå‡½æ•° =====
        def infer_base(text_in, lang_in, ref_audio_path, ref_text_in, xvec_only_in,
                       max_new_tokens_in, temperature_in, top_k_in, top_p_in, repetition_penalty_in):
            if state["tts"] is None:
                return None, "è¯·å…ˆåŠ è½½æ¨¡å‹"
            if not ref_audio_path:
                return None, "è¯·å…ˆä¸Šä¼ å‚è€ƒéŸ³é¢‘"
            if not xvec_only_in and not (ref_text_in or "").strip():
                return None, "ICL æ¨¡å¼éœ€è¦å‚è€ƒæ–‡æœ¬"

            gen_kwargs = _collect_gen_kwargs(
                _coerce_int(max_new_tokens_in, 1024),
                _coerce_float(temperature_in, 0.8),
                _coerce_int(top_k_in, 50),
                _coerce_float(top_p_in, 0.9),
                _coerce_float(repetition_penalty_in, 1.05),
            )
            logits_processor = _build_logits_processor(state.get("sanitize_logits", False))
            if logits_processor is not None:
                gen_kwargs["logits_processor"] = logits_processor
                gen_kwargs["subtalker_dosample"] = False
            wavs, sr = state["tts"].generate_voice_clone(
                text=text_in,
                language=_maybe_auto_language(lang_in),
                ref_audio=ref_audio_path,
                ref_text=ref_text_in,
                x_vector_only_mode=bool(xvec_only_in),
                **gen_kwargs,
            )
            saved_path = ""
            if save_audio:
                saved_path = _save_wav(wavs[0], sr, output_dir, "base")
            status_msg = f"OK{f' | Saved: {saved_path}' if saved_path else ''}"
            return (sr, wavs[0]), status_msg

        base_gen_btn.click(
            infer_base,
            inputs=[base_text, base_language, base_ref_audio, base_ref_text, base_xvec_only,
                    base_max_tokens, base_temp, base_top_k, base_top_p, base_rep_pen],
            outputs=[base_audio_out, base_status]
        )

        def infer_custom(text_in, lang_in, speaker_in, instruct_in,
                         max_new_tokens_in, temperature_in, top_k_in, top_p_in, repetition_penalty_in):
            if state["tts"] is None:
                return None, "è¯·å…ˆåŠ è½½æ¨¡å‹"
            if not speaker_in:
                return None, "è¯·é€‰æ‹© speaker"

            gen_kwargs = _collect_gen_kwargs(
                _coerce_int(max_new_tokens_in, 1024),
                _coerce_float(temperature_in, 0.8),
                _coerce_int(top_k_in, 50),
                _coerce_float(top_p_in, 0.9),
                _coerce_float(repetition_penalty_in, 1.05),
            )
            logits_processor = _build_logits_processor(state.get("sanitize_logits", False))
            if logits_processor is not None:
                gen_kwargs["logits_processor"] = logits_processor
                gen_kwargs["subtalker_dosample"] = False
            wavs, sr = state["tts"].generate_custom_voice(
                text=text_in,
                language=_maybe_auto_language(lang_in),
                speaker=speaker_in,
                instruct=instruct_in,
                **gen_kwargs,
            )
            saved_path = ""
            if save_audio:
                saved_path = _save_wav(wavs[0], sr, output_dir, "custom")
            status_msg = f"OK{f' | Saved: {saved_path}' if saved_path else ''}"
            return (sr, wavs[0]), status_msg

        custom_gen_btn.click(
            infer_custom,
            inputs=[custom_text, custom_language, custom_speaker, custom_instruct,
                    custom_max_tokens, custom_temp, custom_top_k, custom_top_p, custom_rep_pen],
            outputs=[custom_audio_out, custom_status]
        )

        def infer_design(text_in, lang_in, instruct_in,
                         max_new_tokens_in, temperature_in, top_k_in, top_p_in, repetition_penalty_in):
            if state["tts"] is None:
                return None, "è¯·å…ˆåŠ è½½æ¨¡å‹"

            gen_kwargs = _collect_gen_kwargs(
                _coerce_int(max_new_tokens_in, 1024),
                _coerce_float(temperature_in, 0.8),
                _coerce_int(top_k_in, 50),
                _coerce_float(top_p_in, 0.9),
                _coerce_float(repetition_penalty_in, 1.05),
            )
            logits_processor = _build_logits_processor(state.get("sanitize_logits", False))
            if logits_processor is not None:
                gen_kwargs["logits_processor"] = logits_processor
                gen_kwargs["subtalker_dosample"] = False
            t0 = time.time()
            wavs, sr = state["tts"].generate_voice_design(
                text=text_in,
                language=_maybe_auto_language(lang_in),
                instruct=instruct_in,
                **gen_kwargs,
            )
            latency = time.time() - t0
            audio_len = len(wavs[0]) / sr if sr > 0 else 0
            _get_tracker().record(latency, len(text_in), audio_len)

            saved_path = ""
            if save_audio:
                saved_path = _save_wav(wavs[0], sr, output_dir, "design")
            rtf = latency / audio_len if audio_len > 0 else 0
            status_msg = f"OK | {latency:.2f}s | RTF {rtf:.2f}{f' | Saved: {saved_path}' if saved_path else ''}"
            return (sr, wavs[0]), status_msg

        design_gen_btn.click(
            infer_design,
            inputs=[design_text, design_language, design_instruct,
                    design_max_tokens, design_temp, design_top_k, design_top_p, design_rep_pen],
            outputs=[design_audio_out, design_status]
        )

    return demo


def _get_local_ip() -> str:
    """è·å–æœ¬æœºå±€åŸŸç½‘ IP åœ°å€"""
    import socket
    try:
        # åˆ›å»ºä¸€ä¸ª UDP socket è¿æ¥åˆ°å¤–éƒ¨åœ°å€ï¼ˆä¸å®é™…å‘é€æ•°æ®ï¼‰
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.connect(("8.8.8.8", 80))
        ip = s.getsockname()[0]
        s.close()
        return ip
    except Exception:
        logger.debug("Cannot detect LAN IP, falling back to 127.0.0.1", exc_info=True)
        return "127.0.0.1"

def _load_trt_tts(args: argparse.Namespace) -> Qwen3TTSModel:
    """Load TensorRT-LLM INT4 engine for Qwen3-TTS."""
    engine_path = (args.engine_path or "").strip()
    tokenizer_dir = (args.tokenizer_dir or "").strip()
    if not engine_path:
        raise ValueError("--engine-path is required when --backend trt")
    if not tokenizer_dir:
        raise ValueError("--tokenizer-dir is required when --backend trt")

    module_name = os.getenv("TRT_QWEN_TTS_MODULE", "trt_qwen_tts")
    logger.info("Loading TRT backend: module=%s, engine=%s, tokenizer=%s",
                module_name, engine_path, tokenizer_dir)
    try:
        mod = importlib.import_module(module_name)
    except Exception as e:
        logger.error("Failed to import TRT module '%s': %s", module_name, e, exc_info=True)
        raise ImportError(
            f"Failed to import TRT backend module '{module_name}'. "
            "Set TRT_QWEN_TTS_MODULE to your module or ensure trt_qwen_tts.py is available."
        ) from e

    if not hasattr(mod, "TRTQwen3TTSModel"):
        logger.error("Module '%s' does not expose TRTQwen3TTSModel", module_name)
        raise ImportError(f"Module '{module_name}' does not expose TRTQwen3TTSModel")

    cls = getattr(mod, "TRTQwen3TTSModel")
    try:
        tts = cls.from_engine(engine_path=engine_path, tokenizer_dir=tokenizer_dir)
    except Exception as e:
        logger.error("TRT engine loading failed (engine=%s): %s", engine_path, e, exc_info=True)
        raise

    logger.info("TRT engine loaded successfully")

    # Ensure model_type is visible to UI
    if not hasattr(tts, "model"):
        from types import SimpleNamespace
        tts.model = SimpleNamespace(tts_model_type="voice_design")
    else:
        if not getattr(tts.model, "tts_model_type", None):
            setattr(tts.model, "tts_model_type", "voice_design")
    return tts


def main(argv=None) -> int:
    parser = build_parser()
    args = parser.parse_args(argv)

    # Initialize logging early â€” before any logger.xxx calls
    setup_logging()
    logger.info("Qwen3-TTS Jetson Gradio app starting")

    # Initialize Jetson hardware monitor (background thread)
    _get_monitor()
    _get_tracker()

    launch_kwargs: Dict[str, Any] = dict(
        server_name=args.ip,
        server_port=args.port,
        share=args.share,
    )
    if args.ssl_certfile:
        launch_kwargs["ssl_certfile"] = args.ssl_certfile
    if args.ssl_keyfile:
        launch_kwargs["ssl_keyfile"] = args.ssl_keyfile

    # æ³¨é‡Šæ‰è‡ªåŠ¨æ‰«æå’ŒåŠ è½½æ¨¡å‹çš„é€»è¾‘
    # ç°åœ¨ç›´æ¥å¯åŠ¨ UIï¼Œè®©ç”¨æˆ·åœ¨ç•Œé¢ä¸­é€‰æ‹©ä¸‹è½½å’ŒåŠ è½½æ¨¡å‹
    # checkpoint = args.checkpoint
    #
    # # å¦‚æœæ²¡æœ‰æŒ‡å®š checkpointï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹
    # if checkpoint is None or args.auto_detect:
    #     print("æ­£åœ¨æ‰«ææœ¬åœ°æ¨¡å‹ç¼“å­˜...")
    #     auto_path = _auto_detect_model()
    #     if auto_path:
    #         print(f"æ£€æµ‹åˆ°å·²ç¼“å­˜çš„æ¨¡å‹: {auto_path}")
    #         if checkpoint is None:
    #             checkpoint = auto_path
    #     else:
    #         print("æœªæ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹ï¼Œå°†æ˜¾ç¤ºä¸‹è½½ç•Œé¢ã€‚")
    #
    # # å¦‚æœä»ç„¶æ²¡æœ‰ checkpointï¼Œæ˜¾ç¤ºä¸‹è½½ç•Œé¢
    # if checkpoint is None:
    #     print("å¯åŠ¨æ¨¡å‹ä¸‹è½½ç•Œé¢...")
    #     demo = build_download_ui()
    #     demo.queue(default_concurrency_limit=int(args.concurrency)).launch(**launch_kwargs)
    #     return 0
    #
    # # æ£€æŸ¥æŒ‡å®šçš„ checkpoint æ˜¯å¦æœ‰æ•ˆ
    # model_status = _check_model_downloaded(checkpoint)
    # if model_status["status"] in {"local_file", "local_dir_invalid", "cached_invalid", "local_missing"}:
    #     print(f"é”™è¯¯: æŒ‡å®šçš„æ¨¡å‹è·¯å¾„æ— æ•ˆ: {checkpoint}")
    #     if model_status.get("error"):
    #         print(f"åŸå› : {model_status['error']}")
    #     print("è¯·æä¾›æ¨¡å‹ç›®å½•æˆ– HuggingFace repo idã€‚")
    #     return 1
    # if model_status["status"] == "not_cached":
    #     print(f"è­¦å‘Š: æŒ‡å®šçš„æ¨¡å‹ '{checkpoint}' æœªåœ¨æœ¬åœ°æ‰¾åˆ°ã€‚")
    #     print("å°è¯•ä» HuggingFace ä¸‹è½½æ¨¡å‹...")
    #
    # output_dir = _ensure_output_dir(args.output_dir)
    # args.checkpoint = checkpoint  # æ›´æ–° args ä»¥ä¾¿ _load_tts ä½¿ç”¨
    # tts = _load_tts(args)
    # demo = build_demo(tts, checkpoint, output_dir, save_audio=not args.no_save)

    if args.backend == "trt":
        logger.info("å¯åŠ¨ Qwen3-TTS Gradio ç•Œé¢ (TRT backend)")
        output_dir = _ensure_output_dir(args.output_dir)
        tts = _load_trt_tts(args)
        demo = build_demo(
            tts,
            checkpoint=args.engine_path or "",
            output_dir=output_dir,
            save_audio=not args.no_save,
            force_model_type=args.model_type or "voice_design",
        )
    else:
        logger.info("å¯åŠ¨ Qwen3-TTS Gradio ç•Œé¢ (torch backend)")
        logger.info("æ¨¡å‹å°†åœ¨ç•Œé¢ä¸­é€‰æ‹©ååŠ è½½")
        demo = build_lazy_demo(args)

    # è·å–å¹¶æ˜¾ç¤ºè®¿é—®åœ°å€
    local_ip = _get_local_ip()
    protocol = "https" if args.ssl_certfile else "http"
    logger.info("Gradio æœåŠ¡å¯åŠ¨ä¸­ â€” æœ¬æœº: %s://127.0.0.1:%s  å±€åŸŸç½‘: %s://%s:%s%s",
                protocol, args.port, protocol, local_ip, args.port,
                " (å…¬ç½‘åˆ†äº«å·²å¯ç”¨)" if args.share else "")

    demo.queue(default_concurrency_limit=int(args.concurrency)).launch(**launch_kwargs)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
