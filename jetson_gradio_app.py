#!/usr/bin/env python3
# coding=utf-8
"""
Minimal Gradio app for deploying Qwen3-TTS on Jetson Orin.

Supports Base / CustomVoice / VoiceDesign models based on the checkpoint.
"""

from __future__ import annotations

import argparse
import os
import sys
import time
import uuid
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import gradio as gr
import numpy as np
import torch
from transformers.generation.logits_process import LogitsProcessorList

class _NanClampLogitsProcessor:
    def __call__(self, input_ids, scores):
        if torch.isnan(scores).any() or torch.isinf(scores).any():
            scores = torch.nan_to_num(scores, nan=-1e4, posinf=-1e4, neginf=-1e4)
        return scores

def _add_local_qwen_repo() -> None:
    env_root = os.getenv("QWEN3_TTS_ROOT")
    candidates = []
    if env_root:
        candidates.append(env_root)
    candidates.append(str(Path(__file__).resolve().parent / "Qwen3-TTS"))
    for c in candidates:
        if c and Path(c).is_dir():
            if c not in sys.path:
                sys.path.insert(0, c)
            return


_add_local_qwen_repo()

from qwen_tts import Qwen3TTSModel


def _format_bytes(num_bytes: Optional[int]) -> str:
    if num_bytes is None:
        return "unknown"
    if num_bytes <= 0:
        return "0 B"
    units = ["B", "KiB", "MiB", "GiB", "TiB"]
    size = float(num_bytes)
    for u in units:
        if size < 1024 or u == units[-1]:
            return f"{size:.2f} {u}"
        size /= 1024.0
    return f"{size:.2f} TiB"


def _get_default_download_root() -> Path:
    env_root = os.getenv("QWEN3_TTS_DOWNLOAD_DIR")
    if env_root:
        return Path(env_root).expanduser().resolve()
    return Path.cwd().resolve()


def _get_default_download_dir(repo_id: str) -> Path:
    safe_repo = repo_id.replace("/", "__")
    return _get_default_download_root() / safe_repo


def _coerce_int(value: Optional[float], default: int) -> int:
    try:
        if value is None:
            return default
        if isinstance(value, float) and np.isnan(value):
            return default
        return int(value)
    except Exception:
        return default


def _coerce_float(value: Optional[float], default: float) -> float:
    try:
        if value is None:
            return default
        if isinstance(value, float) and np.isnan(value):
            return default
        return float(value)
    except Exception:
        return default


def _read_meminfo() -> Dict[str, int]:
    meminfo: Dict[str, int] = {}
    try:
        with open("/proc/meminfo", "r", encoding="utf-8") as f:
            for line in f:
                parts = line.strip().split(":")
                if len(parts) != 2:
                    continue
                key = parts[0].strip()
                value = parts[1].strip().split()
                if not value:
                    continue
                try:
                    meminfo[key] = int(value[0]) * 1024
                except ValueError:
                    continue
    except OSError:
        pass
    return meminfo


def _check_swap() -> Dict[str, Any]:
    result: Dict[str, Any] = {"enabled": False, "total_bytes": 0, "used_bytes": 0, "entries": []}
    try:
        with open("/proc/swaps", "r", encoding="utf-8") as f:
            lines = [ln.strip() for ln in f.readlines() if ln.strip()]
    except OSError:
        return result

    if len(lines) <= 1:
        return result

    total_kib = 0
    used_kib = 0
    entries = []
    for ln in lines[1:]:
        parts = ln.split()
        if len(parts) < 5:
            continue
        filename, _, size_kib, used_kib_entry, _ = parts[:5]
        try:
            size_kib = int(size_kib)
            used_kib_entry = int(used_kib_entry)
        except ValueError:
            continue
        total_kib += size_kib
        used_kib += used_kib_entry
        entries.append({"path": filename, "size_kib": size_kib, "used_kib": used_kib_entry})

    result["enabled"] = total_kib > 0
    result["total_bytes"] = total_kib * 1024
    result["used_bytes"] = used_kib * 1024
    result["entries"] = entries
    return result


def _check_cuda_mem() -> Dict[str, Optional[int]]:
    if not torch.cuda.is_available():
        return {"total": None, "free": None, "used": None}
    if hasattr(torch.cuda, "mem_get_info"):
        free, total = torch.cuda.mem_get_info()
        return {"total": int(total), "free": int(free), "used": int(total - free)}
    return {"total": None, "free": None, "used": None}


def _cuda_brief_info() -> str:
    if not torch.cuda.is_available():
        return "CUDA available: False"
    try:
        idx = torch.cuda.current_device()
        name = torch.cuda.get_device_name(idx)
    except Exception:
        idx = "unknown"
        name = "unknown"
    return f"CUDA available: True | current_device={idx} | name={name}"


def _infer_model_device(model: torch.nn.Module) -> str:
    try:
        p = next(model.parameters())
        return str(p.device)
    except Exception:
        dev = getattr(model, "device", None)
        return str(dev) if dev is not None else "unknown"


def _move_speech_tokenizer(st, device: str, dtype: Optional[torch.dtype]) -> None:
    if st is None:
        return
    target_device = torch.device(device)
    target_dtype = dtype
    if target_device.type == "cpu" and target_dtype in (torch.float16, torch.bfloat16):
        target_dtype = torch.float32
    if getattr(st, "model", None) is not None:
        if target_dtype is not None:
            st.model = st.model.to(device=target_device, dtype=target_dtype)
        else:
            st.model = st.model.to(device=target_device)
    st.device = target_device


# æ”¯æŒçš„ Qwen3-TTS æ¨¡å‹åˆ—è¡¨
SUPPORTED_MODELS = [
    "Qwen/Qwen3-TTS-12Hz-0.6B-Base",
    "Qwen/Qwen3-TTS-12Hz-0.6B-CustomVoice",
    "Qwen/Qwen3-TTS-12Hz-0.6B-VoiceDesign",
    "Qwen/Qwen3-TTS-25Hz-0.6B-Base",
    "Qwen/Qwen3-TTS-25Hz-0.6B-CustomVoice",
    "Qwen/Qwen3-TTS-25Hz-0.6B-VoiceDesign",
]


def _validate_model_dir(model_dir: Path) -> Tuple[bool, bool]:
    has_config = (model_dir / "config.json").exists()
    has_weights = any(model_dir.glob("*.safetensors")) or any(model_dir.glob("*.bin"))
    return has_config, has_weights


def _get_hf_cache_dirs() -> List[Path]:
    """è·å– HuggingFace å¸¸è§çš„ç¼“å­˜ç›®å½•åˆ—è¡¨"""
    cache_dirs = []

    # 1. ç¯å¢ƒå˜é‡æŒ‡å®šçš„ç¼“å­˜ç›®å½•
    hf_home = os.getenv("HF_HOME")
    if hf_home:
        cache_dirs.append(Path(hf_home) / "hub")

    hf_cache = os.getenv("HUGGINGFACE_HUB_CACHE")
    if hf_cache:
        cache_dirs.append(Path(hf_cache))

    transformers_cache = os.getenv("TRANSFORMERS_CACHE")
    if transformers_cache:
        cache_dirs.append(Path(transformers_cache))

    # 2. é»˜è®¤ç¼“å­˜ç›®å½• (åŸºäºå½“å‰ç”¨æˆ·ä¸»ç›®å½•)
    home = Path.home()
    default_dirs = [
        home / ".cache" / "huggingface" / "hub",
        home / ".cache" / "huggingface" / "transformers",
        home / ".huggingface" / "hub",
    ]
    cache_dirs.extend(default_dirs)

    # 3. å»é‡å¹¶åªè¿”å›å­˜åœ¨ä¸”å¯è®¿é—®çš„ç›®å½•
    seen = set()
    result = []
    for d in cache_dirs:
        try:
            d = d.resolve()
            if d not in seen and d.exists():
                seen.add(d)
                result.append(d)
        except (PermissionError, OSError):
            # è·³è¿‡æ— æƒé™è®¿é—®çš„ç›®å½•
            continue
    return result


def _find_model_in_hf_cache(repo_id: str) -> Optional[Path]:
    """åœ¨ HuggingFace ç¼“å­˜ç›®å½•ä¸­æŸ¥æ‰¾æ¨¡å‹"""
    # HuggingFace ç¼“å­˜ä½¿ç”¨ models--org--name æ ¼å¼
    cache_folder_name = "models--" + repo_id.replace("/", "--")

    for cache_dir in _get_hf_cache_dirs():
        try:
            model_cache = cache_dir / cache_folder_name
            if model_cache.exists():
                # æ£€æŸ¥ snapshots ç›®å½•
                snapshots = model_cache / "snapshots"
                if snapshots.exists():
                    # è¿”å›æœ€æ–°çš„ snapshot
                    snapshot_dirs = sorted(snapshots.iterdir(), key=lambda x: x.stat().st_mtime, reverse=True)
                    for snap in snapshot_dirs:
                        if snap.is_dir():
                            # éªŒè¯æ˜¯å¦æœ‰å¿…è¦çš„æ–‡ä»¶
                            has_config, has_weights = _validate_model_dir(snap)
                            if has_config and has_weights:
                                return snap
        except (PermissionError, OSError):
            # è·³è¿‡æ— æƒé™è®¿é—®çš„ç›®å½•
            continue
    return None


def _check_model_downloaded(checkpoint: str) -> Dict[str, Any]:
    # 1. æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„
    if os.path.exists(checkpoint):
        ckpt_path = Path(checkpoint)
        if ckpt_path.is_file():
            return {
                "status": "local_file",
                "path": str(ckpt_path),
                "error": "checkpoint å¿…é¡»æ˜¯ç›®å½•æˆ– HuggingFace repo id",
            }
        if ckpt_path.is_dir():
            has_config, has_weights = _validate_model_dir(ckpt_path)
            status = "local_dir" if (has_config and has_weights) else "local_dir_invalid"
            return {
                "status": status,
                "path": str(ckpt_path),
                "has_config": has_config,
                "has_weights": has_weights,
                "error": "æ¨¡å‹ç›®å½•ç¼ºå°‘ config.json æˆ–æƒé‡æ–‡ä»¶" if status == "local_dir_invalid" else None,
            }
        return {"status": "local_missing", "path": str(ckpt_path)}

    # 2. å°è¯•ä½¿ç”¨ huggingface_hub çš„ local_files_only æ¨¡å¼
    try:
        from huggingface_hub import snapshot_download

        repo_dir = snapshot_download(
            repo_id=checkpoint,
            local_files_only=True,
            allow_patterns=["*.safetensors", "*.bin", "config.json", "generation_config.json", "*.json"],
        )
        repo_dir = Path(repo_dir)
        has_config, has_weights = _validate_model_dir(repo_dir)
        if has_config and has_weights:
            return {"status": "cached", "path": str(repo_dir)}
        return {
            "status": "cached_invalid",
            "path": str(repo_dir),
            "has_config": has_config,
            "has_weights": has_weights,
            "error": "ç¼“å­˜æ¨¡å‹ç¼ºå°‘ config.json æˆ–æƒé‡æ–‡ä»¶",
        }
    except Exception:
        pass

    # 3. æ£€æŸ¥å½“å‰ç›®å½•ä¸‹çš„é»˜è®¤ä¸‹è½½ä½ç½®
    try:
        local_dir = _get_default_download_dir(checkpoint)
        if local_dir.exists():
            has_config, has_weights = _validate_model_dir(local_dir)
            status = "local_dir" if (has_config and has_weights) else "local_dir_invalid"
            return {
                "status": status,
                "path": str(local_dir),
                "has_config": has_config,
                "has_weights": has_weights,
                "error": "æ¨¡å‹ç›®å½•ç¼ºå°‘ config.json æˆ–æƒé‡æ–‡ä»¶" if status == "local_dir_invalid" else None,
            }
    except Exception:
        pass

    # 4. æ‰‹åŠ¨æœç´¢ HuggingFace ç¼“å­˜ç›®å½•
    found_path = _find_model_in_hf_cache(checkpoint)
    if found_path:
        return {"status": "cached", "path": str(found_path)}

    return {"status": "not_cached", "error": "æ¨¡å‹æœªåœ¨æœ¬åœ°æ‰¾åˆ°"}


def _ensure_output_dir(output_dir: str) -> str:
    path = Path(output_dir).expanduser().resolve()
    path.mkdir(parents=True, exist_ok=True)
    return str(path)


def _save_wav(wav: np.ndarray, sr: int, output_dir: str, prefix: str) -> str:
    ts = time.strftime("%Y%m%d_%H%M%S")
    name = f"{prefix}_{ts}_{uuid.uuid4().hex[:6]}.wav"
    out_path = os.path.join(output_dir, name)
    try:
        import soundfile as sf

        sf.write(out_path, wav, sr)
    except Exception:
        try:
            from scipy.io import wavfile

            wav_int16 = np.clip(wav, -1.0, 1.0)
            wav_int16 = (wav_int16 * 32767.0).astype(np.int16)
            wavfile.write(out_path, sr, wav_int16)
        except Exception as e:
            raise RuntimeError(f"Failed to save audio: {e}") from e
    return out_path


def _system_check_summary(checkpoint: str, output_dir: str) -> str:
    meminfo = _read_meminfo()
    mem_total = meminfo.get("MemTotal")
    mem_avail = meminfo.get("MemAvailable")
    mem_used = mem_total - mem_avail if mem_total and mem_avail else None

    swap = _check_swap()
    cuda = _check_cuda_mem()
    model = _check_model_downloaded(checkpoint)

    lines = []
    lines.append(f"æ¨¡å‹æ£€æŸ¥: {model.get('status')}")
    if model.get("status") in {"local_dir", "local_dir_invalid", "cached_invalid"}:
        lines.append(f"- è·¯å¾„: {model.get('path')}")
        lines.append(f"- config.json: {model.get('has_config')}")
        lines.append(f"- æƒé‡æ–‡ä»¶: {model.get('has_weights')}")
        if model.get("error"):
            lines.append(f"- é”™è¯¯: {model.get('error')}")
    elif model.get("status") in {"local_file", "cached"}:
        lines.append(f"- è·¯å¾„: {model.get('path')}")
        if model.get("error"):
            lines.append(f"- é”™è¯¯: {model.get('error')}")
    elif model.get("status") == "not_cached":
        lines.append("- æœªåœ¨æœ¬åœ°ç¼“å­˜æ£€æµ‹åˆ°æ¨¡å‹ï¼Œå¯æå‰ç¦»çº¿ä¸‹è½½")

    lines.append(f"å†…å­˜: { _format_bytes(mem_used) } / { _format_bytes(mem_total) } (used/total)")
    if swap["enabled"]:
        lines.append(f"Swap: { _format_bytes(swap['used_bytes']) } / { _format_bytes(swap['total_bytes']) }")
    else:
        lines.append("Swap: æœªå¯ç”¨")
    if cuda["total"]:
        lines.append(
            f"CUDA æ˜¾å­˜: { _format_bytes(cuda['used']) } / { _format_bytes(cuda['total']) }"
        )
    lines.append(f"è¾“å‡ºç›®å½•: {output_dir}")
    return "\n".join(lines)


def _dtype_from_str(s: str) -> torch.dtype:
    s = (s or "").strip().lower()
    if s in ("bf16", "bfloat16"):
        return torch.bfloat16
    if s in ("fp16", "float16", "half"):
        return torch.float16
    if s in ("fp32", "float32"):
        return torch.float32
    raise ValueError(f"Unsupported torch dtype: {s}. Use bfloat16/float16/float32.")


def _maybe_auto_language(lang: str) -> str:
    lang = (lang or "").strip()
    return lang if lang else "Auto"


def _collect_gen_kwargs(max_new_tokens, temperature, top_k, top_p, repetition_penalty) -> Dict[str, Any]:
    mapping = {
        "max_new_tokens": max_new_tokens,
        "temperature": temperature,
        "top_k": top_k,
        "top_p": top_p,
        "repetition_penalty": repetition_penalty,
    }
    return {k: v for k, v in mapping.items() if v is not None}


def _build_logits_processor(enabled: bool) -> Optional[LogitsProcessorList]:
    if not enabled:
        return None
    return LogitsProcessorList([_NanClampLogitsProcessor()])


def _load_tts(args: argparse.Namespace) -> Qwen3TTSModel:
    dtype = _dtype_from_str(args.dtype)
    attn_impl = None if args.no_flash_attn else "flash_attention_2"
    device_map = args.device
    if isinstance(device_map, str) and device_map.startswith("cuda:"):
        parts = device_map.split(":", 1)
        if len(parts) == 2 and parts[1].isdigit():
            try:
                torch.cuda.set_device(int(parts[1]))
            except Exception:
                pass
        device_map = "cuda"
    print(f"[Load] device_map={device_map} (from '{args.device}') | dtype={dtype} | flash_attn={'on' if attn_impl else 'off'}")
    print(f"[Load] { _cuda_brief_info() }")

    # åœ¨åŠ è½½å‰æ¸…ç† GPU ç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        import gc
        gc.collect()

    use_staged = bool(getattr(args, "staged_load", False))
    tokenizer_on_cpu = bool(getattr(args, "tokenizer_on_cpu", False))
    print(f"[Load] staged_load={'on' if use_staged else 'off'} | tokenizer_on_cpu={'on' if tokenizer_on_cpu else 'off'}")

    if use_staged and device_map == "cuda":
        print("[Load] staged_load path: CPU -> dtype -> GPU")
        tts = Qwen3TTSModel.from_pretrained(
            args.checkpoint,
            device_map="cpu",
            torch_dtype=dtype if dtype not in (torch.float16, torch.bfloat16) else torch.float32,
            attn_implementation=attn_impl,
            low_cpu_mem_usage=True,
        )
        if dtype in (torch.float16, torch.bfloat16):
            tts.model = tts.model.to(dtype=dtype)
        tts.model = tts.model.to("cuda")
        tts.device = torch.device("cuda")
        if hasattr(tts.model, "device"):
            tts.model.device = torch.device("cuda")
        if hasattr(tts.model, "speech_tokenizer"):
            st = getattr(tts.model, "speech_tokenizer", None)
            if tokenizer_on_cpu:
                _move_speech_tokenizer(st, "cpu", dtype)
            else:
                _move_speech_tokenizer(st, "cuda", dtype)
        return tts

    tts = Qwen3TTSModel.from_pretrained(
        args.checkpoint,
        device_map=device_map,
        torch_dtype=dtype,
        attn_implementation=attn_impl,
        low_cpu_mem_usage=True,  # å‡å°‘åŠ è½½æ—¶çš„å†…å­˜å³°å€¼
    )
    if hasattr(tts.model, "speech_tokenizer"):
        st = getattr(tts.model, "speech_tokenizer", None)
        if tokenizer_on_cpu:
            _move_speech_tokenizer(st, "cpu", dtype)
    return tts


def _build_base_ui(tts: Qwen3TTSModel, output_dir: str, save_audio: bool):
    with gr.Tab("è¯­éŸ³å…‹éš† (Base)"):
        with gr.Row():
            with gr.Column(scale=3):
                text = gr.Textbox(label="åˆæˆæ–‡æœ¬", lines=4, placeholder="è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬...", info="æ”¯æŒä¸­è‹±æ–‡æ··åˆ")
                with gr.Row():
                    language = gr.Dropdown(label="è¯­è¨€", choices=["Auto", "Chinese", "English"], value="Auto", allow_custom_value=True, scale=1)
                    xvec_only = gr.Checkbox(label="ä»…ä½¿ç”¨éŸ³è‰²ç‰¹å¾ (æ— éœ€å‚è€ƒæ–‡æœ¬)", value=False, scale=2)
            with gr.Column(scale=2):
                ref_audio = gr.Audio(label="å‚è€ƒéŸ³é¢‘", type="filepath", sources=["upload", "microphone"])
                ref_text = gr.Textbox(label="å‚è€ƒéŸ³é¢‘æ–‡æœ¬", lines=2, placeholder="è¾“å…¥å‚è€ƒéŸ³é¢‘ä¸­è¯´çš„å†…å®¹...", info="å…³é—­ã€Œä»…éŸ³è‰²ã€æ—¶å¿…å¡«")

        with gr.Accordion("ç”Ÿæˆå‚æ•°", open=False):
            with gr.Row():
                max_new_tokens = gr.Slider(label="max_new_tokens", minimum=256, maximum=4096, value=1024, step=64)
                temperature = gr.Slider(label="temperature", minimum=0.1, maximum=2.0, value=0.8, step=0.05)
            with gr.Row():
                top_k = gr.Slider(label="top_k", minimum=1, maximum=100, value=50, step=1)
                top_p = gr.Slider(label="top_p", minimum=0.1, maximum=1.0, value=0.9, step=0.05)
                repetition_penalty = gr.Slider(label="repetition_penalty", minimum=1.0, maximum=2.0, value=1.05, step=0.01)

        gen_btn = gr.Button("ç”Ÿæˆè¯­éŸ³", variant="primary")
        with gr.Row():
            audio_out = gr.Audio(label="ç”Ÿæˆç»“æœ", type="numpy", scale=3)
            status = gr.Textbox(label="çŠ¶æ€", lines=2, scale=1)

        def _infer_base(
            text_in: str,
            lang_in: str,
            ref_audio_path: Optional[str],
            ref_text_in: str,
            xvec_only_in: bool,
            max_new_tokens_in: float,
            temperature_in: float,
            top_k_in: float,
            top_p_in: float,
            repetition_penalty_in: float,
        ) -> Tuple[Optional[Tuple[int, Any]], str]:
            if not ref_audio_path:
                return None, "è¯·å…ˆä¸Šä¼ å‚è€ƒéŸ³é¢‘"
            if not xvec_only_in and not (ref_text_in or "").strip():
                return None, "ICL æ¨¡å¼éœ€è¦å‚è€ƒæ–‡æœ¬"

            gen_kwargs = _collect_gen_kwargs(
                _coerce_int(max_new_tokens_in, 1024),
                _coerce_float(temperature_in, 0.8),
                _coerce_int(top_k_in, 50),
                _coerce_float(top_p_in, 0.9),
                _coerce_float(repetition_penalty_in, 1.05),
            )
            wavs, sr = tts.generate_voice_clone(
                text=text_in,
                language=_maybe_auto_language(lang_in),
                ref_audio=ref_audio_path,
                ref_text=ref_text_in,
                x_vector_only_mode=bool(xvec_only_in),
                **gen_kwargs,
            )
            saved_path = ""
            if save_audio:
                saved_path = _save_wav(wavs[0], sr, output_dir, "base")
            status_msg = f"OK{f' | Saved: {saved_path}' if saved_path else ''}"
            return (sr, wavs[0]), status_msg

        gen_btn.click(
            _infer_base,
            inputs=[
                text,
                language,
                ref_audio,
                ref_text,
                xvec_only,
                max_new_tokens,
                temperature,
                top_k,
                top_p,
                repetition_penalty,
            ],
            outputs=[audio_out, status],
        )


def _build_custom_ui(tts: Qwen3TTSModel, output_dir: str, save_audio: bool):
    speakers = tts.model.get_supported_speakers() or []
    with gr.Tab("é¢„è®¾è§’è‰² (CustomVoice)"):
        with gr.Row():
            with gr.Column(scale=2):
                text = gr.Textbox(label="åˆæˆæ–‡æœ¬", lines=4, placeholder="è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬...", info="æ”¯æŒä¸­è‹±æ–‡æ··åˆ")
                language = gr.Dropdown(label="è¯­è¨€", choices=["Auto", "Chinese", "English"], value="Auto", allow_custom_value=True)
            with gr.Column(scale=1):
                speaker = gr.Dropdown(label="é€‰æ‹©è§’è‰²", choices=speakers, value=speakers[0] if speakers else None, info="æ¨¡å‹å†…ç½®çš„é¢„è®¾è¯´è¯äºº")
                instruct = gr.Textbox(label="é£æ ¼æŒ‡ä»¤ (å¯é€‰)", lines=2, placeholder="ä¾‹å¦‚: å¼€å¿ƒåœ°ã€æ‚„æ‚„åœ°ã€å¿«é€Ÿ...")

        with gr.Accordion("ç”Ÿæˆå‚æ•°", open=False):
            with gr.Row():
                max_new_tokens = gr.Slider(label="max_new_tokens", minimum=256, maximum=4096, value=1024, step=64)
                temperature = gr.Slider(label="temperature", minimum=0.1, maximum=2.0, value=0.8, step=0.05)
            with gr.Row():
                top_k = gr.Slider(label="top_k", minimum=1, maximum=100, value=50, step=1)
                top_p = gr.Slider(label="top_p", minimum=0.1, maximum=1.0, value=0.9, step=0.05)
                repetition_penalty = gr.Slider(label="repetition_penalty", minimum=1.0, maximum=2.0, value=1.05, step=0.01)

        gen_btn = gr.Button("ç”Ÿæˆè¯­éŸ³", variant="primary")
        with gr.Row():
            audio_out = gr.Audio(label="ç”Ÿæˆç»“æœ", type="numpy", scale=3)
            status = gr.Textbox(label="çŠ¶æ€", lines=2, scale=1)

        def _infer_custom(
            text_in: str,
            lang_in: str,
            speaker_in: str,
            instruct_in: str,
            max_new_tokens_in: float,
            temperature_in: float,
            top_k_in: float,
            top_p_in: float,
            repetition_penalty_in: float,
        ) -> Tuple[Optional[Tuple[int, Any]], str]:
            if not speaker_in:
                return None, "è¯·é€‰æ‹© speaker"

            gen_kwargs = _collect_gen_kwargs(
                _coerce_int(max_new_tokens_in, 1024),
                _coerce_float(temperature_in, 0.8),
                _coerce_int(top_k_in, 50),
                _coerce_float(top_p_in, 0.9),
                _coerce_float(repetition_penalty_in, 1.05),
            )
            wavs, sr = tts.generate_custom_voice(
                text=text_in,
                language=_maybe_auto_language(lang_in),
                speaker=speaker_in,
                instruct=instruct_in,
                **gen_kwargs,
            )
            saved_path = ""
            if save_audio:
                saved_path = _save_wav(wavs[0], sr, output_dir, "custom")
            status_msg = f"OK{f' | Saved: {saved_path}' if saved_path else ''}"
            return (sr, wavs[0]), status_msg

        gen_btn.click(
            _infer_custom,
            inputs=[
                text,
                language,
                speaker,
                instruct,
                max_new_tokens,
                temperature,
                top_k,
                top_p,
                repetition_penalty,
            ],
            outputs=[audio_out, status],
        )


def _build_voice_design_ui(tts: Qwen3TTSModel, output_dir: str, save_audio: bool):
    with gr.Tab("é£æ ¼è®¾è®¡ (VoiceDesign)"):
        with gr.Row():
            with gr.Column(scale=2):
                text = gr.Textbox(label="åˆæˆæ–‡æœ¬", lines=4, placeholder="è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬...", info="æ”¯æŒä¸­è‹±æ–‡æ··åˆ")
                language = gr.Dropdown(label="è¯­è¨€", choices=["Auto", "Chinese", "English"], value="Auto", allow_custom_value=True)
            with gr.Column(scale=1):
                instruct = gr.Textbox(label="è¯­éŸ³é£æ ¼æè¿°", lines=4, placeholder="æè¿°ä½ æƒ³è¦çš„å£°éŸ³ç‰¹ç‚¹...\nä¾‹å¦‚:\n- æ¸©æŸ”çš„å¥³å£°ï¼Œè¯­é€Ÿç¼“æ…¢\n- ä½æ²‰æœ‰ç£æ€§çš„ç”·å£°", info="ç”¨è‡ªç„¶è¯­è¨€æè¿°å£°éŸ³ç‰¹ç‚¹")

        with gr.Accordion("ç”Ÿæˆå‚æ•°", open=False):
            with gr.Row():
                max_new_tokens = gr.Slider(label="max_new_tokens", minimum=256, maximum=4096, value=1024, step=64)
                temperature = gr.Slider(label="temperature", minimum=0.1, maximum=2.0, value=0.8, step=0.05)
            with gr.Row():
                top_k = gr.Slider(label="top_k", minimum=1, maximum=100, value=50, step=1)
                top_p = gr.Slider(label="top_p", minimum=0.1, maximum=1.0, value=0.9, step=0.05)
                repetition_penalty = gr.Slider(label="repetition_penalty", minimum=1.0, maximum=2.0, value=1.05, step=0.01)

        gen_btn = gr.Button("ç”Ÿæˆè¯­éŸ³", variant="primary")
        with gr.Row():
            audio_out = gr.Audio(label="ç”Ÿæˆç»“æœ", type="numpy", scale=3)
            status = gr.Textbox(label="çŠ¶æ€", lines=2, scale=1)

        def _infer_design(
            text_in: str,
            lang_in: str,
            instruct_in: str,
            max_new_tokens_in: float,
            temperature_in: float,
            top_k_in: float,
            top_p_in: float,
            repetition_penalty_in: float,
        ) -> Tuple[Optional[Tuple[int, Any]], str]:
            gen_kwargs = _collect_gen_kwargs(
                _coerce_int(max_new_tokens_in, 1024),
                _coerce_float(temperature_in, 0.8),
                _coerce_int(top_k_in, 50),
                _coerce_float(top_p_in, 0.9),
                _coerce_float(repetition_penalty_in, 1.05),
            )
            wavs, sr = tts.generate_voice_design(
                text=text_in,
                language=_maybe_auto_language(lang_in),
                instruct=instruct_in,
                **gen_kwargs,
            )
            saved_path = ""
            if save_audio:
                saved_path = _save_wav(wavs[0], sr, output_dir, "design")
            status_msg = f"OK{f' | Saved: {saved_path}' if saved_path else ''}"
            return (sr, wavs[0]), status_msg

        gen_btn.click(
            _infer_design,
            inputs=[
                text,
                language,
                instruct,
                max_new_tokens,
                temperature,
                top_k,
                top_p,
                repetition_penalty,
            ],
            outputs=[audio_out, status],
        )


def _scan_all_cached_models() -> List[Dict[str, Any]]:
    """æ‰«ææ‰€æœ‰å·²ç¼“å­˜çš„ Qwen3-TTS æ¨¡å‹"""
    found = []
    for repo_id in SUPPORTED_MODELS:
        result = _check_model_downloaded(repo_id)
        if result["status"] in ("cached", "local_dir"):
            found.append({
                "repo_id": repo_id,
                "path": result.get("path", ""),
                "status": result["status"],
            })
    return found


def _download_model(repo_id: str, local_dir: Optional[str], progress=gr.Progress()) -> str:
    """ä¸‹è½½æ¨¡å‹åˆ°æŒ‡å®šç›®å½•"""
    try:
        from huggingface_hub import snapshot_download
    except ImportError:
        return "é”™è¯¯: è¯·å…ˆå®‰è£… huggingface_hub: pip install huggingface_hub"

    progress(0, desc=f"å¼€å§‹ä¸‹è½½ {repo_id}...")

    try:
        kwargs = {
            "repo_id": repo_id,
            "allow_patterns": [
                "*.safetensors",
                "*.bin",
                "*.pt",
                "*.npz",
                "*.json",
                "*.jsonl",
                "*.txt",
                "*.model",
                "*.vocab",
                "*.tiktoken",
                "*.spm",
                "*.sentencepiece",
                "*.merges",
            ],
        }
        if local_dir and local_dir.strip():
            local_path = Path(local_dir).expanduser().resolve()
        else:
            local_path = _get_default_download_dir(repo_id)
        local_path.mkdir(parents=True, exist_ok=True)
        kwargs["local_dir"] = str(local_path)

        progress(0.1, desc="æ­£åœ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶...")
        result_path = snapshot_download(**kwargs)
        progress(1.0, desc="ä¸‹è½½å®Œæˆ!")
        return f"ä¸‹è½½æˆåŠŸ!\næ¨¡å‹è·¯å¾„: {result_path}\n\nè¯·é‡å¯åº”ç”¨å¹¶ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤:\npython jetson_gradio_app.py {result_path}"
    except Exception as e:
        return f"ä¸‹è½½å¤±è´¥: {str(e)}"


def build_download_ui() -> gr.Blocks:
    """æ„å»ºæ¨¡å‹ä¸‹è½½ç•Œé¢"""
    with gr.Blocks(title="Qwen3-TTS ä¸‹è½½") as demo:
        gr.Markdown("# Qwen3-TTS æ¨¡å‹ä¸‹è½½")
        gr.Markdown("æ£€æµ‹åˆ°æœ¬åœ°æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Œè¯·å…ˆä¸‹è½½æ¨¡å‹")

        # æ˜¾ç¤ºå·²ç¼“å­˜çš„æ¨¡å‹
        cached_models = _scan_all_cached_models()
        if cached_models:
            with gr.Accordion("å·²æ£€æµ‹åˆ°çš„æœ¬åœ°æ¨¡å‹", open=True):
                cached_info = "\n".join([f"- **{m['repo_id']}**\n  è·¯å¾„: `{m['path']}`" for m in cached_models])
                gr.Markdown(cached_info)
                gr.Markdown("å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ç›´æ¥å¯åŠ¨:")
                for m in cached_models:
                    gr.Code(f"python jetson_gradio_app.py {m['path']}", language="bash")

        # ä¸‹è½½æ–°æ¨¡å‹
        with gr.Accordion("ä¸‹è½½æ–°æ¨¡å‹", open=not cached_models):
            with gr.Group():
                model_choice = gr.Dropdown(
                    label="é€‰æ‹©æ¨¡å‹",
                    choices=SUPPORTED_MODELS,
                    value=SUPPORTED_MODELS[0],
                    info="12Hz æ›´å¿«ï¼Œ25Hz è´¨é‡æ›´é«˜ | Base=è¯­éŸ³å…‹éš†, CustomVoice=é¢„è®¾è§’è‰², VoiceDesign=é£æ ¼æè¿°"
                )

                default_root = _get_default_download_root()
                gr.Markdown(f"ğŸ“ **ä¸‹è½½ç›®å½•**: `{default_root}`")

                use_custom_dir = gr.Checkbox(label="ä½¿ç”¨è‡ªå®šä¹‰ä¸‹è½½ç›®å½•", value=False)
                custom_dir = gr.Textbox(
                    label="è‡ªå®šä¹‰ç›®å½•",
                    placeholder=f"ä¾‹å¦‚: ~/models/Qwen3-TTS-0.6B",
                    visible=False
                )

            def toggle_custom_dir(use_custom):
                return gr.update(visible=use_custom)

            use_custom_dir.change(toggle_custom_dir, inputs=[use_custom_dir], outputs=[custom_dir])

            download_btn = gr.Button("å¼€å§‹ä¸‹è½½", variant="primary")
            download_status = gr.Textbox(label="ä¸‹è½½çŠ¶æ€", lines=5, interactive=False)

            def do_download(model, use_custom, custom_path, progress=gr.Progress()):
                local_dir = custom_path if use_custom and custom_path.strip() else None
                return _download_model(model, local_dir, progress)

            download_btn.click(
                do_download,
                inputs=[model_choice, use_custom_dir, custom_dir],
                outputs=[download_status]
            )

        # æ‰‹åŠ¨æŒ‡å®šæ¨¡å‹è·¯å¾„
        with gr.Accordion("æ‰‹åŠ¨æŒ‡å®šæœ¬åœ°æ¨¡å‹è·¯å¾„", open=False):
            gr.Markdown("å¦‚æœæ¨¡å‹å·²ç»ä¸‹è½½åˆ°å…¶ä»–ä½ç½®ï¼Œå¯ä»¥ç›´æ¥æŒ‡å®šè·¯å¾„:")
            with gr.Row():
                manual_path = gr.Textbox(
                    label="æ¨¡å‹è·¯å¾„",
                    placeholder="ä¾‹å¦‚: /path/to/Qwen3-TTS-12Hz-0.6B-Base",
                    scale=3
                )
                check_btn = gr.Button("æ£€æŸ¥", scale=1)
            check_result = gr.Textbox(label="æ£€æŸ¥ç»“æœ", lines=3, interactive=False)

            def check_manual_path(path):
                if not path or not path.strip():
                    return "è¯·è¾“å…¥è·¯å¾„"
                result = _check_model_downloaded(path.strip())
                if result["status"] in ("local_dir", "cached"):
                    return f"âœ… æ£€æµ‹åˆ°æœ‰æ•ˆæ¨¡å‹!\nè·¯å¾„: {result.get('path', path)}\n\nå¯åŠ¨å‘½ä»¤:\npython jetson_gradio_app.py {result.get('path', path)}"
                return f"âŒ æœªæ£€æµ‹åˆ°æœ‰æ•ˆæ¨¡å‹\nçŠ¶æ€: {result['status']}\né”™è¯¯: {result.get('error', 'è·¯å¾„ä¸å­˜åœ¨æˆ–ç¼ºå°‘å¿…è¦æ–‡ä»¶')}"

            check_btn.click(check_manual_path, inputs=[manual_path], outputs=[check_result])

        gr.Markdown("---")
        with gr.Accordion("ä½¿ç”¨è¯´æ˜", open=False):
            gr.Markdown("""
**æ¨¡å‹ç±»å‹è¯´æ˜:**
- **Base**: è¯­éŸ³å…‹éš†æ¨¡å‹ï¼Œéœ€è¦å‚è€ƒéŸ³é¢‘
- **CustomVoice**: é¢„å®šä¹‰è¯´è¯äººæ¨¡å‹
- **VoiceDesign**: é€šè¿‡æ–‡å­—æè¿°æ§åˆ¶è¯­éŸ³é£æ ¼

**é‡‡æ ·ç‡è¯´æ˜:**
- **12Hz**: æ›´å¿«çš„ç”Ÿæˆé€Ÿåº¦
- **25Hz**: æ›´é«˜çš„éŸ³é¢‘è´¨é‡

ä¸‹è½½å®Œæˆåï¼Œä½¿ç”¨æ˜¾ç¤ºçš„å‘½ä»¤é‡å¯åº”ç”¨ã€‚
            """)

    return demo


def build_demo(tts: Qwen3TTSModel, checkpoint: str, output_dir: str, save_audio: bool) -> gr.Blocks:
    with gr.Blocks(title="Qwen3-TTS") as demo:
        gr.Markdown("# Qwen3-TTS Jetson Orin")
        gr.Markdown("æ–‡æœ¬è½¬è¯­éŸ³æ¼”ç¤º | Text-to-Speech Demo")

        model_type = getattr(tts.model, "tts_model_type", "")
        if model_type == "base":
            _build_base_ui(tts, output_dir, save_audio)
        elif model_type == "custom_voice":
            _build_custom_ui(tts, output_dir, save_audio)
        elif model_type == "voice_design":
            _build_voice_design_ui(tts, output_dir, save_audio)
        else:
            gr.Markdown(f"âš ï¸ ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")

        with gr.Accordion("ç³»ç»ŸçŠ¶æ€", open=False):
            sys_info = gr.Textbox(label="è¯¦ç»†ä¿¡æ¯", lines=6, value=_system_check_summary(checkpoint, output_dir))
            refresh_btn = gr.Button("åˆ·æ–°")

            def _refresh() -> str:
                return _system_check_summary(checkpoint, output_dir)

            refresh_btn.click(_refresh, outputs=[sys_info])

        gr.Markdown("---")
        gr.Markdown(
            "<center style='color: #888; font-size: 0.85em;'>"
            "âš ï¸ ç”Ÿæˆçš„éŸ³é¢‘ä»…ä¾›æ¼”ç¤ºä½¿ç”¨ï¼Œè¯·å‹¿ç”¨äºéæ³•æˆ–æœ‰å®³ç”¨é€”ã€‚"
            "</center>"
        )
    return demo


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="Qwen3-TTS Gradio demo for Jetson Orin.")
    parser.add_argument(
        "checkpoint",
        nargs="?",
        default=None,
        help="Model checkpoint path or HuggingFace repo id. If not provided, will auto-detect or show download UI.",
    )
    parser.add_argument("--device", default="cpu", help="Device for device_map (default: cpu).")
    parser.add_argument(
        "--dtype",
        default="float32",
        choices=["bfloat16", "bf16", "float16", "fp16", "float32", "fp32"],
        help="Torch dtype for loading the model (default: float32).",
    )
    parser.add_argument(
        "--no-flash-attn",
        dest="no_flash_attn",
        action="store_true",
        default=True,
        help="Disable FlashAttention-2 (default: disabled).",
    )
    parser.add_argument(
        "--flash-attn",
        dest="no_flash_attn",
        action="store_false",
        help="Enable FlashAttention-2.",
    )
    parser.add_argument(
        "--staged-load",
        action="store_true",
        default=True,
        help="Staged load for Jetson (CPU -> dtype -> GPU).",
    )
    parser.add_argument(
        "--no-staged-load",
        dest="staged_load",
        action="store_false",
        help="Disable staged load.",
    )
    parser.add_argument(
        "--tokenizer-on-cpu",
        action="store_true",
        default=True,
        help="Keep speech tokenizer on CPU (reduce GPU memory).",
    )
    parser.add_argument(
        "--tokenizer-on-gpu",
        dest="tokenizer_on_cpu",
        action="store_false",
        help="Move speech tokenizer to GPU.",
    )
    parser.add_argument("--ip", default="0.0.0.0", help="Gradio server bind IP.")
    parser.add_argument("--port", type=int, default=8000, help="Gradio server port.")
    parser.add_argument("--share", action="store_true", help="Create a public Gradio link.")
    parser.add_argument("--concurrency", type=int, default=4, help="Gradio queue concurrency.")
    parser.add_argument("--ssl-certfile", default=None, help="Path to SSL cert file for HTTPS.")
    parser.add_argument("--ssl-keyfile", default=None, help="Path to SSL key file for HTTPS.")
    parser.add_argument("--output-dir", default="outputs", help="Directory to save generated audio.")
    parser.add_argument("--no-save", action="store_true", help="Disable saving generated audio.")
    parser.add_argument(
        "--auto-detect",
        action="store_true",
        help="Auto-detect and use the first available cached model.",
    )
    return parser


def _scan_local_models() -> List[Dict[str, Any]]:
    """æ‰«æå½“å‰ç›®å½•ä¸‹å·²ä¸‹è½½çš„æ¨¡å‹"""
    found = []
    for repo_id in SUPPORTED_MODELS:
        local_dir = _get_default_download_dir(repo_id)
        if local_dir.exists():
            has_config, has_weights = _validate_model_dir(local_dir)
            if has_config and has_weights:
                found.append({
                    "repo_id": repo_id,
                    "path": str(local_dir),
                    "status": "local_dir",
                })
    return found


def build_lazy_demo(args: argparse.Namespace) -> gr.Blocks:
    """æ„å»ºå»¶è¿ŸåŠ è½½æ¨¡å‹çš„ç•Œé¢ - å…ˆå¯åŠ¨UIï¼ŒååŠ è½½æ¨¡å‹"""
    # ä½¿ç”¨å¯å˜å®¹å™¨å­˜å‚¨æ¨¡å‹çŠ¶æ€
    state = {"tts": None, "checkpoint": None, "model_type": None, "sanitize_logits": True}
    output_dir = _ensure_output_dir(args.output_dir)
    save_audio = not args.no_save

    # å¯åŠ¨æ—¶æ‰«æå½“å‰ç›®å½•ä¸‹çš„æ¨¡å‹
    local_models = _scan_local_models()
    initial_status = "æ¨¡å‹æœªåŠ è½½ã€‚è¯·é€‰æ‹©æ¨¡å‹åç‚¹å‡»ã€Œä¸‹è½½æ¨¡å‹ã€æˆ–ã€ŒåŠ è½½æ¨¡å‹ã€ã€‚"
    default_model = SUPPORTED_MODELS[0]
    default_path = ""

    if local_models:
        # æ‰¾åˆ°æœ¬åœ°æ¨¡å‹ï¼Œè®¾ç½®ä¸ºé»˜è®¤
        first_local = local_models[0]
        default_model = first_local["repo_id"]
        default_path = first_local["path"]
        model_list = "\n".join([f"  - {m['repo_id']}: {m['path']}" for m in local_models])
        initial_status = f"æ£€æµ‹åˆ°æœ¬åœ°å·²ä¸‹è½½çš„æ¨¡å‹:\n{model_list}\n\nå¯ä»¥ç›´æ¥ç‚¹å‡»ã€ŒåŠ è½½æ¨¡å‹ã€"

    with gr.Blocks(title="Qwen3-TTS") as demo:
        gr.Markdown("# Qwen3-TTS Jetson Orin")
        gr.Markdown("æ–‡æœ¬è½¬è¯­éŸ³æ¼”ç¤º | Text-to-Speech Demo")

        # ===== æ¨¡å‹ç®¡ç†åŒºåŸŸ =====
        with gr.Accordion("æ¨¡å‹ç®¡ç†", open=True):
            with gr.Group():
                # æ¨¡å‹é€‰æ‹© - ç¬¬ä¸€è¡Œ
                model_dropdown = gr.Dropdown(
                    label="é€‰æ‹©æ¨¡å‹",
                    choices=SUPPORTED_MODELS,
                    value=default_model,
                    info="12Hz æ›´å¿«ï¼Œ25Hz è´¨é‡æ›´é«˜ | Base=è¯­éŸ³å…‹éš†, CustomVoice=é¢„è®¾è§’è‰², VoiceDesign=é£æ ¼æè¿°"
                )
                # æœ¬åœ°è·¯å¾„ - ç¬¬äºŒè¡Œ
                local_path_input = gr.Textbox(
                    label="æœ¬åœ°æ¨¡å‹è·¯å¾„ (å¯é€‰)",
                    placeholder="ç•™ç©ºåˆ™ä½¿ç”¨ä¸Šæ–¹é€‰æ‹©çš„æ¨¡å‹ï¼Œæˆ–è¾“å…¥å·²ä¸‹è½½æ¨¡å‹çš„å®Œæ•´è·¯å¾„",
                    value=default_path,
                )

            # é«˜çº§åŠ è½½é€‰é¡¹ - æŠ˜å 
            with gr.Accordion("åŠ è½½é€‰é¡¹", open=False):
                with gr.Row():
                    device_input = gr.Dropdown(
                        label="Device",
                        choices=["cpu", "cuda", "cuda:0", "auto"],
                        value=args.device,
                        allow_custom_value=True,
                        info="è¿è¡Œè®¾å¤‡"
                    )
                    dtype_dropdown = gr.Dropdown(
                        label="ç²¾åº¦ (DType)",
                        choices=["float16", "bfloat16", "float32"],
                        value="float16" if args.dtype in ["fp16", "float16"] else args.dtype,
                        info="float16 æ¨èç”¨äº Jetson"
                    )
                    flash_attn_checkbox = gr.Checkbox(
                        label="FlashAttention-2",
                        value=not args.no_flash_attn,
                        info="éœ€è¦å®‰è£… flash-attn"
                    )
                    sanitize_logits_checkbox = gr.Checkbox(
                        label="Sanitize logits",
                        value=True,
                        info="é¿å… CUDA é‡‡æ ·å‡ºç° NaN/Inf"
                    )
                    staged_load_checkbox = gr.Checkbox(
                        label="Staged load (CPUâ†’GPU)",
                        value=bool(args.staged_load),
                        info="Jetson æ¨èï¼Œé™ä½ GPU å³°å€¼"
                    )
                    tokenizer_cpu_checkbox = gr.Checkbox(
                        label="Tokenizer on CPU",
                        value=bool(args.tokenizer_on_cpu),
                        info="å‡å°‘ GPU å‹åŠ›ï¼ˆå¯èƒ½å˜æ…¢ï¼‰"
                    )

            # æ“ä½œæŒ‰é’®
            with gr.Row():
                download_btn = gr.Button("ä¸‹è½½æ¨¡å‹", variant="secondary", scale=1)
                load_btn = gr.Button("åŠ è½½æ¨¡å‹", variant="primary", scale=2)

            # çŠ¶æ€æ˜¾ç¤º
            model_status_text = gr.Textbox(
                label="çŠ¶æ€",
                lines=3,
                interactive=False,
                value=initial_status,
                            )

            # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²æœ‰æ¨¡å‹
            def check_local_model(repo_id: str, local_path: str) -> str:
                path_to_check = local_path.strip() if local_path.strip() else repo_id
                # å…ˆæ£€æŸ¥å½“å‰ç›®å½•ä¸‹çš„æ¨¡å‹
                local_dir = _get_default_download_dir(path_to_check) if not local_path.strip() else Path(local_path)
                if local_dir.exists():
                    has_config, has_weights = _validate_model_dir(local_dir)
                    if has_config and has_weights:
                        return f"æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹: {local_dir}\nå¯ä»¥ç›´æ¥ç‚¹å‡»ã€ŒåŠ è½½æ¨¡å‹ã€"
                result = _check_model_downloaded(path_to_check)
                if result["status"] in ("cached", "local_dir"):
                    return f"æ£€æµ‹åˆ°ç¼“å­˜æ¨¡å‹: {result.get('path', path_to_check)}\nå¯ä»¥ç›´æ¥ç‚¹å‡»ã€ŒåŠ è½½æ¨¡å‹ã€"
                return f"æ¨¡å‹æœªä¸‹è½½ã€‚ç‚¹å‡»ã€Œä¸‹è½½æ¨¡å‹ã€å¼€å§‹ä¸‹è½½åˆ°å½“å‰ç›®å½•ã€‚"

            model_dropdown.change(
                check_local_model,
                inputs=[model_dropdown, local_path_input],
                outputs=[model_status_text]
            )

            # ä¸‹è½½æ¨¡å‹å‡½æ•°
            def do_download(repo_id: str, local_path: str, progress=gr.Progress()):
                target_path = local_path.strip() if local_path.strip() else None
                return _download_model(repo_id, target_path, progress)

            download_btn.click(
                do_download,
                inputs=[model_dropdown, local_path_input],
                outputs=[model_status_text]
            )

        # ===== TTS ç”ŸæˆåŒºåŸŸ (åˆå§‹éšè—) =====
        with gr.Column(visible=False) as tts_area:
            gr.Markdown("---")

            # Base æ¨¡å¼ UI
            with gr.Tab("è¯­éŸ³å…‹éš† (Base)", visible=False) as base_tab:
                with gr.Row():
                    with gr.Column(scale=3):
                        base_text = gr.Textbox(
                            label="åˆæˆæ–‡æœ¬",
                            lines=4,
                            placeholder="è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬...",
                            info="æ”¯æŒä¸­è‹±æ–‡æ··åˆ"
                        )
                        with gr.Row():
                            base_language = gr.Dropdown(
                                label="è¯­è¨€",
                                choices=["Auto", "Chinese", "English"],
                                value="Auto",
                                allow_custom_value=True,
                                scale=1
                            )
                            base_xvec_only = gr.Checkbox(
                                label="ä»…ä½¿ç”¨éŸ³è‰²ç‰¹å¾ (æ— éœ€å‚è€ƒæ–‡æœ¬)",
                                value=False,
                                scale=2
                            )
                    with gr.Column(scale=2):
                        base_ref_audio = gr.Audio(
                            label="å‚è€ƒéŸ³é¢‘",
                            type="filepath",
                            sources=["upload", "microphone"]
                        )
                        base_ref_text = gr.Textbox(
                            label="å‚è€ƒéŸ³é¢‘æ–‡æœ¬",
                            lines=2,
                            placeholder="è¾“å…¥å‚è€ƒéŸ³é¢‘ä¸­è¯´çš„å†…å®¹...",
                            info="å…³é—­ã€Œä»…éŸ³è‰²ã€æ—¶å¿…å¡«"
                        )

                with gr.Accordion("ç”Ÿæˆå‚æ•°", open=False):
                    with gr.Row():
                        base_max_tokens = gr.Slider(label="max_new_tokens", minimum=256, maximum=4096, value=1024, step=64)
                        base_temp = gr.Slider(label="temperature", minimum=0.1, maximum=2.0, value=0.8, step=0.05)
                    with gr.Row():
                        base_top_k = gr.Slider(label="top_k", minimum=1, maximum=100, value=50, step=1)
                        base_top_p = gr.Slider(label="top_p", minimum=0.1, maximum=1.0, value=0.9, step=0.05)
                        base_rep_pen = gr.Slider(label="repetition_penalty", minimum=1.0, maximum=2.0, value=1.05, step=0.01)

                base_gen_btn = gr.Button("ç”Ÿæˆè¯­éŸ³", variant="primary")

                with gr.Row():
                    base_audio_out = gr.Audio(label="ç”Ÿæˆç»“æœ", type="numpy", scale=3)
                    base_status = gr.Textbox(label="çŠ¶æ€", lines=2, scale=1)

            # CustomVoice æ¨¡å¼ UI
            with gr.Tab("é¢„è®¾è§’è‰² (CustomVoice)", visible=False) as custom_tab:
                with gr.Row():
                    with gr.Column(scale=2):
                        custom_text = gr.Textbox(
                            label="åˆæˆæ–‡æœ¬",
                            lines=4,
                            placeholder="è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬...",
                            info="æ”¯æŒä¸­è‹±æ–‡æ··åˆ"
                        )
                        custom_language = gr.Dropdown(
                            label="è¯­è¨€",
                            choices=["Auto", "Chinese", "English"],
                            value="Auto",
                            allow_custom_value=True
                        )
                    with gr.Column(scale=1):
                        custom_speaker = gr.Dropdown(
                            label="é€‰æ‹©è§’è‰²",
                            choices=[],
                            value=None,
                            info="æ¨¡å‹å†…ç½®çš„é¢„è®¾è¯´è¯äºº"
                        )
                        custom_instruct = gr.Textbox(
                            label="é£æ ¼æŒ‡ä»¤ (å¯é€‰)",
                            lines=2,
                            placeholder="ä¾‹å¦‚: å¼€å¿ƒåœ°ã€æ‚„æ‚„åœ°ã€å¿«é€Ÿ..."
                        )

                with gr.Accordion("ç”Ÿæˆå‚æ•°", open=False):
                    with gr.Row():
                        custom_max_tokens = gr.Slider(label="max_new_tokens", minimum=256, maximum=4096, value=1024, step=64)
                        custom_temp = gr.Slider(label="temperature", minimum=0.1, maximum=2.0, value=0.8, step=0.05)
                    with gr.Row():
                        custom_top_k = gr.Slider(label="top_k", minimum=1, maximum=100, value=50, step=1)
                        custom_top_p = gr.Slider(label="top_p", minimum=0.1, maximum=1.0, value=0.9, step=0.05)
                        custom_rep_pen = gr.Slider(label="repetition_penalty", minimum=1.0, maximum=2.0, value=1.05, step=0.01)

                custom_gen_btn = gr.Button("ç”Ÿæˆè¯­éŸ³", variant="primary")

                with gr.Row():
                    custom_audio_out = gr.Audio(label="ç”Ÿæˆç»“æœ", type="numpy", scale=3)
                    custom_status = gr.Textbox(label="çŠ¶æ€", lines=2, scale=1)

            # VoiceDesign æ¨¡å¼ UI
            with gr.Tab("é£æ ¼è®¾è®¡ (VoiceDesign)", visible=False) as design_tab:
                with gr.Row():
                    with gr.Column(scale=2):
                        design_text = gr.Textbox(
                            label="åˆæˆæ–‡æœ¬",
                            lines=4,
                            placeholder="è¯·è¾“å…¥è¦åˆæˆçš„æ–‡æœ¬...",
                            info="æ”¯æŒä¸­è‹±æ–‡æ··åˆ"
                        )
                        design_language = gr.Dropdown(
                            label="è¯­è¨€",
                            choices=["Auto", "Chinese", "English"],
                            value="Auto",
                            allow_custom_value=True
                        )
                    with gr.Column(scale=1):
                        design_instruct = gr.Textbox(
                            label="è¯­éŸ³é£æ ¼æè¿°",
                            lines=4,
                            placeholder="æè¿°ä½ æƒ³è¦çš„å£°éŸ³ç‰¹ç‚¹...\nä¾‹å¦‚:\n- æ¸©æŸ”çš„å¥³å£°ï¼Œè¯­é€Ÿç¼“æ…¢\n- ä½æ²‰æœ‰ç£æ€§çš„ç”·å£°\n- æ´»æ³¼çš„æ’­éŸ³è…”",
                            info="ç”¨è‡ªç„¶è¯­è¨€æè¿°å£°éŸ³ç‰¹ç‚¹"
                        )

                with gr.Accordion("ç”Ÿæˆå‚æ•°", open=False):
                    with gr.Row():
                        design_max_tokens = gr.Slider(label="max_new_tokens", minimum=256, maximum=4096, value=1024, step=64)
                        design_temp = gr.Slider(label="temperature", minimum=0.1, maximum=2.0, value=0.8, step=0.05)
                    with gr.Row():
                        design_top_k = gr.Slider(label="top_k", minimum=1, maximum=100, value=50, step=1)
                        design_top_p = gr.Slider(label="top_p", minimum=0.1, maximum=1.0, value=0.9, step=0.05)
                        design_rep_pen = gr.Slider(label="repetition_penalty", minimum=1.0, maximum=2.0, value=1.05, step=0.01)

                design_gen_btn = gr.Button("ç”Ÿæˆè¯­éŸ³", variant="primary")

                with gr.Row():
                    design_audio_out = gr.Audio(label="ç”Ÿæˆç»“æœ", type="numpy", scale=3)
                    design_status = gr.Textbox(label="çŠ¶æ€", lines=2, scale=1)

            # ç³»ç»ŸçŠ¶æ€æ”¾åœ¨æœ€å
            with gr.Accordion("ç³»ç»ŸçŠ¶æ€", open=False):
                sys_info = gr.Textbox(label="è¯¦ç»†ä¿¡æ¯", lines=6, value="")
                refresh_btn = gr.Button("åˆ·æ–°")

        gr.Markdown("---")
        gr.Markdown(
            "<center style='color: #888; font-size: 0.85em;'>"
            "âš ï¸ ç”Ÿæˆçš„éŸ³é¢‘ä»…ä¾›æ¼”ç¤ºä½¿ç”¨ï¼Œè¯·å‹¿ç”¨äºéæ³•æˆ–æœ‰å®³ç”¨é€”ã€‚"
            "</center>"
        )

        # ===== åŠ è½½æ¨¡å‹é€»è¾‘ =====
        def load_model_fn(repo_id: str, local_path: str, device_in: str, dtype_in: str, flash_attn_in: bool, sanitize_logits_in: bool, staged_load_in: bool, tokenizer_cpu_in: bool):
            nonlocal state
            try:
                # åº”ç”¨å½“å‰ UI ä¸­çš„åŠ è½½å‚æ•°
                args.device = (device_in or "").strip() or args.device
                args.dtype = (dtype_in or "").strip() or args.dtype
                args.no_flash_attn = not bool(flash_attn_in)
                args.staged_load = bool(staged_load_in)
                args.tokenizer_on_cpu = bool(tokenizer_cpu_in)
                if args.device.lower().startswith("cpu") and args.dtype.lower() in ("bfloat16", "bf16", "float16", "fp16"):
                    print("[Info] CPU æ¨¡å¼ä¸‹å°† dtype è‡ªåŠ¨åˆ‡æ¢ä¸º float32ï¼Œä»¥é¿å… NaN/Infã€‚")
                    args.dtype = "float32"
                state["sanitize_logits"] = bool(sanitize_logits_in)

                # ç¡®å®šæ¨¡å‹è·¯å¾„
                if local_path.strip():
                    checkpoint = local_path.strip()
                else:
                    # å…ˆæ£€æŸ¥å½“å‰ç›®å½•ä¸‹è½½çš„æ¨¡å‹
                    local_dir = _get_default_download_dir(repo_id)
                    if local_dir.exists():
                        has_config, has_weights = _validate_model_dir(local_dir)
                        if has_config and has_weights:
                            checkpoint = str(local_dir)
                        else:
                            checkpoint = repo_id
                    else:
                        checkpoint = repo_id

                # åŠ è½½æ¨¡å‹
                args.checkpoint = checkpoint
                tts = _load_tts(args)
                model_type = getattr(tts.model, "tts_model_type", "")
                print(f"[Load] model_type={model_type} | model_device={_infer_model_device(tts.model)}")
                try:
                    st = getattr(tts.model, "speech_tokenizer", None)
                    st_model = getattr(st, "model", None) if st is not None else None
                    if st_model is not None:
                        print(f"[Load] speech_tokenizer_device={_infer_model_device(st_model)}")
                except Exception:
                    pass

                state["tts"] = tts
                state["checkpoint"] = checkpoint
                state["model_type"] = model_type

                # è·å– speakers (ä»… custom_voice æ¨¡å¼)
                speakers = []
                if model_type == "custom_voice":
                    speakers = tts.model.get_supported_speakers() or []

                status_msg = (
                    f"æ¨¡å‹åŠ è½½æˆåŠŸ!\nè·¯å¾„: {checkpoint}\nç±»å‹: {model_type}\n"
                    f"device={args.device} | dtype={args.dtype} | flash_attn={'on' if not args.no_flash_attn else 'off'} | "
                    f"sanitize_logits={'on' if state.get('sanitize_logits', False) else 'off'} | staged_load={'on' if args.staged_load else 'off'} | "
                    f"tokenizer_on_cpu={'on' if args.tokenizer_on_cpu else 'off'}"
                )
                sys_check = _system_check_summary(checkpoint, output_dir)

                # è¿”å› UI æ›´æ–°
                return (
                    status_msg,  # model_status_text
                    gr.update(visible=True),  # tts_area
                    sys_check,  # sys_info
                    gr.update(visible=(model_type == "base")),  # base_tab
                    gr.update(visible=(model_type == "custom_voice")),  # custom_tab
                    gr.update(visible=(model_type == "voice_design")),  # design_tab
                    gr.update(choices=speakers, value=speakers[0] if speakers else None),  # custom_speaker
                )
            except Exception as e:
                error_msg = f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
                return (
                    error_msg,
                    gr.update(visible=False),
                    "",
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(visible=False),
                    gr.update(choices=[], value=None),
                )

        load_btn.click(
            load_model_fn,
            inputs=[model_dropdown, local_path_input, device_input, dtype_dropdown, flash_attn_checkbox, sanitize_logits_checkbox, staged_load_checkbox, tokenizer_cpu_checkbox],
            outputs=[model_status_text, tts_area, sys_info, base_tab, custom_tab, design_tab, custom_speaker]
        )

        # åˆ·æ–°ç³»ç»Ÿæ£€æŸ¥
        def refresh_sys_check():
            if state["checkpoint"]:
                return _system_check_summary(state["checkpoint"], output_dir)
            return "æ¨¡å‹æœªåŠ è½½"

        refresh_btn.click(refresh_sys_check, outputs=[sys_info])

        # ===== TTS ç”Ÿæˆå‡½æ•° =====
        def infer_base(text_in, lang_in, ref_audio_path, ref_text_in, xvec_only_in,
                       max_new_tokens_in, temperature_in, top_k_in, top_p_in, repetition_penalty_in):
            if state["tts"] is None:
                return None, "è¯·å…ˆåŠ è½½æ¨¡å‹"
            if not ref_audio_path:
                return None, "è¯·å…ˆä¸Šä¼ å‚è€ƒéŸ³é¢‘"
            if not xvec_only_in and not (ref_text_in or "").strip():
                return None, "ICL æ¨¡å¼éœ€è¦å‚è€ƒæ–‡æœ¬"

            gen_kwargs = _collect_gen_kwargs(
                _coerce_int(max_new_tokens_in, 1024),
                _coerce_float(temperature_in, 0.8),
                _coerce_int(top_k_in, 50),
                _coerce_float(top_p_in, 0.9),
                _coerce_float(repetition_penalty_in, 1.05),
            )
            logits_processor = _build_logits_processor(state.get("sanitize_logits", False))
            if logits_processor is not None:
                gen_kwargs["logits_processor"] = logits_processor
                gen_kwargs["subtalker_dosample"] = False
            wavs, sr = state["tts"].generate_voice_clone(
                text=text_in,
                language=_maybe_auto_language(lang_in),
                ref_audio=ref_audio_path,
                ref_text=ref_text_in,
                x_vector_only_mode=bool(xvec_only_in),
                **gen_kwargs,
            )
            saved_path = ""
            if save_audio:
                saved_path = _save_wav(wavs[0], sr, output_dir, "base")
            status_msg = f"OK{f' | Saved: {saved_path}' if saved_path else ''}"
            return (sr, wavs[0]), status_msg

        base_gen_btn.click(
            infer_base,
            inputs=[base_text, base_language, base_ref_audio, base_ref_text, base_xvec_only,
                    base_max_tokens, base_temp, base_top_k, base_top_p, base_rep_pen],
            outputs=[base_audio_out, base_status]
        )

        def infer_custom(text_in, lang_in, speaker_in, instruct_in,
                         max_new_tokens_in, temperature_in, top_k_in, top_p_in, repetition_penalty_in):
            if state["tts"] is None:
                return None, "è¯·å…ˆåŠ è½½æ¨¡å‹"
            if not speaker_in:
                return None, "è¯·é€‰æ‹© speaker"

            gen_kwargs = _collect_gen_kwargs(
                _coerce_int(max_new_tokens_in, 1024),
                _coerce_float(temperature_in, 0.8),
                _coerce_int(top_k_in, 50),
                _coerce_float(top_p_in, 0.9),
                _coerce_float(repetition_penalty_in, 1.05),
            )
            logits_processor = _build_logits_processor(state.get("sanitize_logits", False))
            if logits_processor is not None:
                gen_kwargs["logits_processor"] = logits_processor
                gen_kwargs["subtalker_dosample"] = False
            wavs, sr = state["tts"].generate_custom_voice(
                text=text_in,
                language=_maybe_auto_language(lang_in),
                speaker=speaker_in,
                instruct=instruct_in,
                **gen_kwargs,
            )
            saved_path = ""
            if save_audio:
                saved_path = _save_wav(wavs[0], sr, output_dir, "custom")
            status_msg = f"OK{f' | Saved: {saved_path}' if saved_path else ''}"
            return (sr, wavs[0]), status_msg

        custom_gen_btn.click(
            infer_custom,
            inputs=[custom_text, custom_language, custom_speaker, custom_instruct,
                    custom_max_tokens, custom_temp, custom_top_k, custom_top_p, custom_rep_pen],
            outputs=[custom_audio_out, custom_status]
        )

        def infer_design(text_in, lang_in, instruct_in,
                         max_new_tokens_in, temperature_in, top_k_in, top_p_in, repetition_penalty_in):
            if state["tts"] is None:
                return None, "è¯·å…ˆåŠ è½½æ¨¡å‹"

            gen_kwargs = _collect_gen_kwargs(
                _coerce_int(max_new_tokens_in, 1024),
                _coerce_float(temperature_in, 0.8),
                _coerce_int(top_k_in, 50),
                _coerce_float(top_p_in, 0.9),
                _coerce_float(repetition_penalty_in, 1.05),
            )
            logits_processor = _build_logits_processor(state.get("sanitize_logits", False))
            if logits_processor is not None:
                gen_kwargs["logits_processor"] = logits_processor
                gen_kwargs["subtalker_dosample"] = False
            wavs, sr = state["tts"].generate_voice_design(
                text=text_in,
                language=_maybe_auto_language(lang_in),
                instruct=instruct_in,
                **gen_kwargs,
            )
            saved_path = ""
            if save_audio:
                saved_path = _save_wav(wavs[0], sr, output_dir, "design")
            status_msg = f"OK{f' | Saved: {saved_path}' if saved_path else ''}"
            return (sr, wavs[0]), status_msg

        design_gen_btn.click(
            infer_design,
            inputs=[design_text, design_language, design_instruct,
                    design_max_tokens, design_temp, design_top_k, design_top_p, design_rep_pen],
            outputs=[design_audio_out, design_status]
        )

    return demo


def _get_local_ip() -> str:
    """è·å–æœ¬æœºå±€åŸŸç½‘ IP åœ°å€"""
    import socket
    try:
        # åˆ›å»ºä¸€ä¸ª UDP socket è¿æ¥åˆ°å¤–éƒ¨åœ°å€ï¼ˆä¸å®é™…å‘é€æ•°æ®ï¼‰
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.connect(("8.8.8.8", 80))
        ip = s.getsockname()[0]
        s.close()
        return ip
    except Exception:
        return "127.0.0.1"


def main(argv=None) -> int:
    parser = build_parser()
    args = parser.parse_args(argv)

    launch_kwargs: Dict[str, Any] = dict(
        server_name=args.ip,
        server_port=args.port,
        share=args.share,
    )
    if args.ssl_certfile:
        launch_kwargs["ssl_certfile"] = args.ssl_certfile
    if args.ssl_keyfile:
        launch_kwargs["ssl_keyfile"] = args.ssl_keyfile

    # æ³¨é‡Šæ‰è‡ªåŠ¨æ‰«æå’ŒåŠ è½½æ¨¡å‹çš„é€»è¾‘
    # ç°åœ¨ç›´æ¥å¯åŠ¨ UIï¼Œè®©ç”¨æˆ·åœ¨ç•Œé¢ä¸­é€‰æ‹©ä¸‹è½½å’ŒåŠ è½½æ¨¡å‹
    # checkpoint = args.checkpoint
    #
    # # å¦‚æœæ²¡æœ‰æŒ‡å®š checkpointï¼Œå°è¯•è‡ªåŠ¨æ£€æµ‹
    # if checkpoint is None or args.auto_detect:
    #     print("æ­£åœ¨æ‰«ææœ¬åœ°æ¨¡å‹ç¼“å­˜...")
    #     auto_path = _auto_detect_model()
    #     if auto_path:
    #         print(f"æ£€æµ‹åˆ°å·²ç¼“å­˜çš„æ¨¡å‹: {auto_path}")
    #         if checkpoint is None:
    #             checkpoint = auto_path
    #     else:
    #         print("æœªæ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹ï¼Œå°†æ˜¾ç¤ºä¸‹è½½ç•Œé¢ã€‚")
    #
    # # å¦‚æœä»ç„¶æ²¡æœ‰ checkpointï¼Œæ˜¾ç¤ºä¸‹è½½ç•Œé¢
    # if checkpoint is None:
    #     print("å¯åŠ¨æ¨¡å‹ä¸‹è½½ç•Œé¢...")
    #     demo = build_download_ui()
    #     demo.queue(default_concurrency_limit=int(args.concurrency)).launch(**launch_kwargs)
    #     return 0
    #
    # # æ£€æŸ¥æŒ‡å®šçš„ checkpoint æ˜¯å¦æœ‰æ•ˆ
    # model_status = _check_model_downloaded(checkpoint)
    # if model_status["status"] in {"local_file", "local_dir_invalid", "cached_invalid", "local_missing"}:
    #     print(f"é”™è¯¯: æŒ‡å®šçš„æ¨¡å‹è·¯å¾„æ— æ•ˆ: {checkpoint}")
    #     if model_status.get("error"):
    #         print(f"åŸå› : {model_status['error']}")
    #     print("è¯·æä¾›æ¨¡å‹ç›®å½•æˆ– HuggingFace repo idã€‚")
    #     return 1
    # if model_status["status"] == "not_cached":
    #     print(f"è­¦å‘Š: æŒ‡å®šçš„æ¨¡å‹ '{checkpoint}' æœªåœ¨æœ¬åœ°æ‰¾åˆ°ã€‚")
    #     print("å°è¯•ä» HuggingFace ä¸‹è½½æ¨¡å‹...")
    #
    # output_dir = _ensure_output_dir(args.output_dir)
    # args.checkpoint = checkpoint  # æ›´æ–° args ä»¥ä¾¿ _load_tts ä½¿ç”¨
    # tts = _load_tts(args)
    # demo = build_demo(tts, checkpoint, output_dir, save_audio=not args.no_save)

    print("å¯åŠ¨ Qwen3-TTS Gradio ç•Œé¢...")
    print("æ¨¡å‹å°†åœ¨ç•Œé¢ä¸­é€‰æ‹©ååŠ è½½ã€‚")
    demo = build_lazy_demo(args)

    # è·å–å¹¶æ˜¾ç¤ºè®¿é—®åœ°å€
    local_ip = _get_local_ip()
    protocol = "https" if args.ssl_certfile else "http"
    print("\n" + "=" * 50)
    print("Gradio æœåŠ¡å¯åŠ¨ä¸­...")
    print(f"æœ¬æœºè®¿é—®: {protocol}://127.0.0.1:{args.port}")
    print(f"å±€åŸŸç½‘è®¿é—®: {protocol}://{local_ip}:{args.port}")
    if args.share:
        print("å…¬ç½‘é“¾æ¥å°†åœ¨ä¸‹æ–¹æ˜¾ç¤º...")
    print("=" * 50 + "\n")

    demo.queue(default_concurrency_limit=int(args.concurrency)).launch(**launch_kwargs)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
